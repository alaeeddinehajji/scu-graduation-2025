{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EMG Gesture Classification: Comparative Analysis of Deep Learning Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, List, Dict\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset from a CSV file.\n",
        "df = pd.read_csv(\"data/processed/EMG-data.csv\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Set device for PyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwfX5Whfb6ak",
        "outputId": "6a06f3ca-a2c5-411b-ab9f-ebabb757395c"
      },
      "outputs": [],
      "source": [
        "def print_color(text: str, color: str) -> None:\n",
        "    \"\"\"\n",
        "    Prints text in specified ANSI color for better readability in notebooks.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Text to be printed\n",
        "        color (str): Color name ('red', 'green', 'yellow', 'blue', 'magenta')\n",
        "    \"\"\"\n",
        "    colors = {\n",
        "        \"red\": \"\\033[91m\",\n",
        "        \"green\": \"\\033[92m\",\n",
        "        \"yellow\": \"\\033[93m\",\n",
        "        \"blue\": \"\\033[94m\",\n",
        "        \"magenta\": \"\\033[95m\"\n",
        "    }\n",
        "    print(f\"{colors.get(color, '')}{text}\\033[0m\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_emg_channels(data: np.ndarray, title: str = \"EMG Signals\", \n",
        "                      sample_idx: int = 0, channels: int = 8) -> None:\n",
        "    \"\"\"\n",
        "    Plot EMG channels from a window of data.\n",
        "    \n",
        "    Args:\n",
        "        data (np.ndarray): EMG data array with shape (windows, time_steps, channels)\n",
        "        title (str): Title for the plot\n",
        "        sample_idx (int): Index of the window to plot\n",
        "        channels (int): Number of EMG channels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 16))\n",
        "    window_data = data[sample_idx]\n",
        "    for i in range(channels):\n",
        "        plt.subplot(channels, 1, i + 1)\n",
        "        plt.plot(window_data[:, i])\n",
        "        plt.title(f'{title}, Channel {i+1}')\n",
        "        plt.xlabel('Time Step')\n",
        "        plt.ylabel('Amplitude')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_sliding_windows(data: np.ndarray, labels: np.ndarray, \n",
        "                          window_size: int, step_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Create sliding windows from EMG data for time series analysis.\n",
        "    \n",
        "    Args:\n",
        "        data (np.ndarray): EMG data as a numpy array\n",
        "        labels (np.ndarray): Corresponding labels\n",
        "        window_size (int): Size of each window\n",
        "        step_size (int): Step size between consecutive windows\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: Windowed data and corresponding labels\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    \n",
        "    for i in range(0, len(data) - window_size + 1, step_size):\n",
        "        X_list.append(data[i:i + window_size])\n",
        "        # Use the most frequent label in the window\n",
        "        y_list.append(labels[i + window_size // 2])  # Take the middle point's label\n",
        "    \n",
        "    return np.array(X_list), np.array(y_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adjust gesture labels to be zero-indexed\n",
        "df['gesture'] = df['gesture'] - 1\n",
        "\n",
        "# Count number of unique gestures\n",
        "num_classes = df['gesture'].nunique()\n",
        "\n",
        "# Explore dataset\n",
        "print_color(\"Head of DataFrame:\", \"green\")\n",
        "print(df.head())\n",
        "\n",
        "print_color(\"Shape of DataFrame:\", \"green\")\n",
        "print(df.shape)\n",
        "\n",
        "# Check for null values\n",
        "null_count = df.isnull().sum()\n",
        "print_color(\"Null values in each column:\", \"yellow\")\n",
        "print(null_count)\n",
        "\n",
        "# Dataset properties\n",
        "print_color(\"Unique gestures (zero-indexed):\", \"blue\")\n",
        "print(sorted(df[\"gesture\"].unique()))\n",
        "\n",
        "print_color(\"Unique subjects:\", \"blue\")\n",
        "print(sorted(df[\"subject\"].unique()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set window size and step size for slicing the data\n",
        "WINDOW_SIZE = 100  # Number of samples per window\n",
        "STEP_SIZE = 50     # Interval between consecutive windows\n",
        "\n",
        "# Extract channel data\n",
        "channel_cols = [f'Channel_{i}' for i in range(1, 9)]\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "# Group the data by gesture and create windows\n",
        "for gesture_id in sorted(df[\"gesture\"].unique()):\n",
        "    gesture_df = df[df[\"gesture\"] == gesture_id]\n",
        "    gesture_data = gesture_df[channel_cols].values\n",
        "    \n",
        "    # Generate overlapping windows of data\n",
        "    for start_idx in range(0, len(gesture_data) - WINDOW_SIZE + 1, STEP_SIZE):\n",
        "        window_data = gesture_data[start_idx:start_idx + WINDOW_SIZE]\n",
        "        X_list.append(window_data)\n",
        "        y_list.append(gesture_id)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X_array = np.array(X_list)\n",
        "y_array = np.array(y_list)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_array, dtype=torch.long)\n",
        "\n",
        "# Move tensors to device\n",
        "X_tensor = X_tensor.to(device)\n",
        "y_tensor = y_tensor.to(device)\n",
        "\n",
        "# Save tensors for later use (optional)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "torch.save(X_tensor, \"data/X_emg.pt\")\n",
        "torch.save(y_tensor, \"data/y_emg.pt\")\n",
        "\n",
        "# Display shapes and stats\n",
        "print_color(\"Shape of X_tensor:\", \"red\")\n",
        "print(X_tensor.shape)\n",
        "print_color(\"Shape of y_tensor:\", \"red\")\n",
        "print(y_tensor.shape)\n",
        "\n",
        "# Print data statistics\n",
        "print_color(\"Data statistics in X_tensor:\", \"green\")\n",
        "print(f\"Mean: {torch.mean(X_tensor).item():.4f}\")\n",
        "print(f\"Standard Deviation: {torch.std(X_tensor).item():.4f}\")\n",
        "print(f\"Max value: {torch.max(X_tensor).item():.4f}\")\n",
        "print(f\"Min value: {torch.min(X_tensor).item():.4f}\")\n",
        "\n",
        "# Load the windowed data (if previously saved)\n",
        "# X_tensor = torch.load(\"data/X_emg.pt\")\n",
        "X_numpy = X_tensor.cpu().numpy()\n",
        "\n",
        "# Plot a sample window\n",
        "plot_emg_channels(X_numpy, title=\"Sample Window\", sample_idx=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training, validation, and test sets\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(\n",
        "    X_tensor.cpu().numpy(),\n",
        "    y_tensor.cpu().numpy(),\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y_tensor.cpu().numpy()  # Ensure class balance in splits\n",
        ")\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(\n",
        "    x_temp, y_temp, \n",
        "    test_size=0.5, \n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y_temp  # Maintain class balance\n",
        ")\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Output the shapes of the datasets\n",
        "print(\"Dataset splits:\")\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Verify class distribution in splits\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "for c in range(num_classes):\n",
        "    print(f\"Class {c}: {torch.sum(y_train == c).item()} samples\")\n",
        "\n",
        "print(\"\\nClass distribution in validation set:\")\n",
        "for c in range(num_classes):\n",
        "    print(f\"Class {c}: {torch.sum(y_val == c).item()} samples\")\n",
        "\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "for c in range(num_classes):\n",
        "    print(f\"Class {c}: {torch.sum(y_test == c).item()} samples\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move tensors to CPU for normalization\n",
        "x_train_cpu = x_train\n",
        "x_val_cpu = x_val\n",
        "x_test_cpu = x_test\n",
        "\n",
        "# Reshape for easier normalization\n",
        "train_reshaped = x_train_cpu.numpy().reshape(-1, x_train_cpu.shape[-1])\n",
        "val_reshaped = x_val_cpu.numpy().reshape(-1, x_val_cpu.shape[-1])\n",
        "test_reshaped = x_test_cpu.numpy().reshape(-1, x_test_cpu.shape[-1])\n",
        "\n",
        "# Calculate mean and std from training data only (to prevent data leakage)\n",
        "mean = np.mean(train_reshaped, axis=0)\n",
        "std = np.std(train_reshaped, axis=0)\n",
        "\n",
        "# Prevent division by zero\n",
        "std[std < 1e-10] = 1.0\n",
        "\n",
        "# Normalize data\n",
        "train_reshaped = (train_reshaped - mean) / std\n",
        "val_reshaped = (val_reshaped - mean) / std\n",
        "test_reshaped = (test_reshaped - mean) / std\n",
        "\n",
        "# Reshape back to original format\n",
        "x_train_norm = torch.tensor(train_reshaped.reshape(x_train_cpu.shape), dtype=torch.float32)\n",
        "x_val_norm = torch.tensor(val_reshaped.reshape(x_val_cpu.shape), dtype=torch.float32)\n",
        "x_test_norm = torch.tensor(test_reshaped.reshape(x_test_cpu.shape), dtype=torch.float32)\n",
        "\n",
        "# Move normalized data to device\n",
        "x_train_norm = x_train_norm.to(device)\n",
        "x_val_norm = x_val_norm.to(device)\n",
        "x_test_norm = x_test_norm.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_val = y_val.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# Print shapes to confirm dimensions\n",
        "print(\"\\nNormalized data shapes:\")\n",
        "print(f\"x_train_norm shape: {x_train_norm.shape}\")\n",
        "print(f\"x_val_norm shape: {x_val_norm.shape}\")\n",
        "print(f\"x_test_norm shape: {x_test_norm.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert labels to one-hot encoding for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert class vectors to one-hot encoded tensors\n",
        "y_train_oh = F.one_hot(y_train, num_classes=num_classes).float()\n",
        "y_val_oh = F.one_hot(y_val, num_classes=num_classes).float()\n",
        "y_test_oh = F.one_hot(y_test, num_classes=num_classes).float()\n",
        "\n",
        "# Output the shapes to verify correct dimensions\n",
        "print(\"\\nOne-hot encoded label shapes:\")\n",
        "print(f\"y_train_oh shape: {y_train_oh.shape}\")\n",
        "print(f\"y_val_oh shape: {y_val_oh.shape}\")\n",
        "print(f\"y_test_oh shape: {y_test_oh.shape}\")\n",
        "\n",
        "# Print an example to verify encoding\n",
        "print(f\"\\nExample one-hot encoded label (class {y_test[0].item()}):\")\n",
        "print(y_test_oh[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iif_Z1SfreCz",
        "outputId": "65443e24-31a3-460c-f1da-d3c7b4f1ee5b"
      },
      "outputs": [],
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, win_size: int, num_channels: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(num_channels, 128, kernel_size=5, padding=2),  # Increased filters\n",
        "            nn.BatchNorm1d(128),  # Added batch norm\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            \n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),  # Added extra layer\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            \n",
        "            nn.Conv1d(256, 512, kernel_size=3, padding=1),  # Deeper architecture\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1),  # Better than fixed pooling\n",
        "            \n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),  # Increased dropout\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Permute to (batch_size, channels, time_steps)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return self.model(x)\n",
        "\n",
        "# Example usage:\n",
        "model = CNN1D(win_size=100, num_channels=8, num_classes=6).to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZS7r_2GreCz",
        "outputId": "da847c99-d50a-4858-ac03-8f7ff320e12f"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, win_size, num_channels, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm1 = nn.LSTM(input_size=num_channels, hidden_size=64, batch_first=True, dropout=0.2, bidirectional=False)\n",
        "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True, dropout=0.2, bidirectional=False)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(64, 128)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, num_classes)  # No softmax (handled by CrossEntropyLoss)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Permute/transpose the input to (batch_size, num_channels, win_size)\n",
        "        # x = x.permute(2, 0, 1)  # (win_size, batch_size, num_channels)\n",
        "\n",
        "        x, _ = self.lstm1(x)  # First LSTM layer\n",
        "        x, _ = self.lstm2(x)  # Second LSTM layer (last time step)\n",
        "\n",
        "        x = x[:, -1, :]  # Take only the last time step output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)  # Raw logits\n",
        "\n",
        "        return x  # No softmax, use CrossEntropyLoss\n",
        "# Example usage:\n",
        "model = LSTMModel(win_size=100, num_channels=8, num_classes=6).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAqIY389reCz",
        "outputId": "b8ef111e-504d-4540-95f7-5f720cda0e40"
      },
      "outputs": [],
      "source": [
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, win_size, num_channels, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "       \n",
        "        # --- CNN Block (inspired by your CNN1D model) ---\n",
        "        self.cnn_block = nn.Sequential(\n",
        "            nn.Conv1d(num_channels, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.2),\n",
        "           \n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "       \n",
        "        # Calculate CNN output size\n",
        "        self.cnn_output_size = win_size // 4  # After two MaxPool layers\n",
        "       \n",
        "        # --- Attention Layer for CNN features ---\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv1d(256, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "       \n",
        "        # --- LSTM Block with bidirectional option ---\n",
        "        self.lstm1 = nn.LSTM(\n",
        "            input_size=256,\n",
        "            hidden_size=128,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "       \n",
        "        self.lstm2 = nn.LSTM(\n",
        "            input_size=256,  # 128*2 due to bidirectional\n",
        "            hidden_size=128,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "       \n",
        "        # --- Fully Connected Layers ---\n",
        "        self.fc_block = nn.Sequential(\n",
        "            nn.Linear(256, 512),  # 128*2 due to bidirectional\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "       \n",
        "    def forward(self, x):\n",
        "        # Always ensure input is in the correct shape for CNN: (batch_size, channels, time_steps)\n",
        "        # Expected input shape: (batch_size, time_steps, channels)\n",
        "        x = x.permute(0, 2, 1)  # Transform to (batch_size, channels, time_steps)\n",
        "           \n",
        "        # CNN feature extraction\n",
        "        cnn_features = self.cnn_block(x)\n",
        "       \n",
        "        # Apply attention to CNN features\n",
        "        attention_weights = self.attention(cnn_features)\n",
        "        attended_features = cnn_features * attention_weights\n",
        "       \n",
        "        # Reshape for LSTM: (batch, time, features)\n",
        "        lstm_input = attended_features.permute(0, 2, 1)\n",
        "       \n",
        "        # LSTM processing\n",
        "        lstm_out1, _ = self.lstm1(lstm_input)\n",
        "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
        "       \n",
        "        # Global context representation\n",
        "        # Combine max and average pooling across time dimension\n",
        "        max_pool = torch.max(lstm_out2, dim=1)[0]\n",
        "        avg_pool = torch.mean(lstm_out2, dim=1)\n",
        "        combined_features = max_pool + avg_pool\n",
        "       \n",
        "        # Final classification\n",
        "        output = self.fc_block(combined_features)\n",
        "       \n",
        "        return output\n",
        "   \n",
        "    def initialize_weights(self):\n",
        "        \"\"\"Initialize model weights for better convergence\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight_ih' in name:\n",
        "                        nn.init.xavier_uniform_(param.data)\n",
        "                    elif 'weight_hh' in name:\n",
        "                        nn.init.orthogonal_(param.data)\n",
        "                    elif 'bias' in name:\n",
        "                        param.data.fill_(0)\n",
        "\n",
        "# Model instantiation\n",
        "model = CNNLSTMModel(win_size=100, num_channels=8, num_classes=6).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "5e86rHNsy3KB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 10  # Increase for better training\n",
        "BATCH_SIZE = 32\n",
        "learning_rate = 1e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "UzyDgADu0fAf"
      },
      "outputs": [],
      "source": [
        "def train_model(model, model_name, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor):\n",
        "    \"\"\"\n",
        "    Trains the given model and records both training and validation performance.\n",
        "\n",
        "    Parameters:\n",
        "    - model: PyTorch model\n",
        "    - model_name: String (name of the model)\n",
        "    - x_train_tensor, y_train_tensor: Training data\n",
        "    - x_val_tensor, y_val_tensor: Validation data\n",
        "\n",
        "    Returns:\n",
        "    - history: Dictionary containing train/validation loss & accuracy per epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    class_counts = np.bincount(y_train.cpu().numpy())\n",
        "    class_weights = 1. / class_counts\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "   \n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # AdamW with weight decay\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for i in range(0, len(x_train_tensor), BATCH_SIZE):\n",
        "            batch_x = x_train_tensor[i:i+BATCH_SIZE].to(device)\n",
        "            batch_y = y_train_tensor[i:i+BATCH_SIZE].to(device)\n",
        "\n",
        "            # if model_name in [ \"CNN+LSTM\"]: # Apply permute for CNN1D and CNN+LSTM\n",
        "            #   batch_x = batch_x.permute(0, 2, 1)  # Adjust shape for CNN1D\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == batch_y.argmax(1)).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(x_train_tensor)\n",
        "        epoch_acc = correct / total\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "\n",
        "        # Validation Phase\n",
        "        val_loss, val_acc = evaluate_model(model, x_val_tensor, y_val_tensor, is_test=False)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Step the scheduler every 10 epochs\n",
        "        if (epoch + 1) % 25 == 0:  # Step the scheduler every 10 epochs\n",
        "            scheduler.step(val_loss)  # Reduces learning rate based on validation loss\n",
        "\n",
        "        print(f\"[{model_name}] Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return history  # Return history for plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, x_test_tensor, y_test_tensor, is_test=True):\n",
        "    \"\"\"\n",
        "    Evaluates the given model on test or validation data.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained PyTorch model\n",
        "    - x_test_tensor, y_test_tensor: Test/Validation data\n",
        "    - is_test: If True, prints classification report & confusion matrix\n",
        "\n",
        "    Returns:\n",
        "    - test_loss: Float, test/validation loss\n",
        "    - test_accuracy: Float, test/validation accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_test_tensor = x_test_tensor.to(device)\n",
        "        # if isinstance(model, (CNNLSTMModel)):  # Check for both model types\n",
        "        #   x_test_tensor = x_test_tensor.permute(0, 2, 1)  # Adjust shape for CNN1D\n",
        "\n",
        "        y_test_tensor = y_test_tensor.to(device)\n",
        "\n",
        "        test_outputs = model(x_test_tensor)\n",
        "        test_pred = test_outputs.argmax(1)  # Get predicted class indices\n",
        "        y_test_indices = y_test_tensor.argmax(1)  # Convert one-hot labels to indices\n",
        "\n",
        "        test_loss = nn.CrossEntropyLoss()(test_outputs, y_test_tensor)\n",
        "        test_accuracy = (test_pred == y_test_indices).sum().item() / y_test_indices.size(0)\n",
        "\n",
        "    if is_test:\n",
        "        print(f\"\\n[Test] Loss: {test_loss:.4f} | Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"\\nClassification Report:\\n\", classification_report(y_test_indices.cpu().numpy(), test_pred.cpu().numpy()))\n",
        "        print(f\"Confusion Matrix:\\n\", confusion_matrix(y_test_indices.cpu().numpy(), test_pred.cpu().numpy()))\n",
        "\n",
        "    return test_loss.item(), test_accuracy  # Return for further analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Train and evaluate Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9Nd3X6J0imJ"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate CNN\n",
        "cnn_model = CNN1D(win_size=x_train_norm.shape[1], num_channels=x_train_norm.shape[2], num_classes=y_train_oh.shape[1])\n",
        "\n",
        "# Train the model and store training history (include validation data)\n",
        "history_cnn = train_model(cnn_model, \"CNN1D\", x_train_norm, y_train_oh, x_val_norm, y_val_oh)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss_cnn, test_acc_cnn = evaluate_model(cnn_model, x_test_norm, y_test_oh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2XQDhCE0icb",
        "outputId": "879a0df9-2051-41c7-dc01-c021a9dd9be3"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate LSTM model\n",
        "lstm_model = LSTMModel(win_size=x_train_norm.shape[1], num_channels=x_train_norm.shape[2], num_classes=y_train_oh.shape[1])\n",
        "\n",
        "# Train the model and store training history (include validation data)\n",
        "history_lstm = train_model(lstm_model, \"LSTM\", x_train_norm, y_train_oh, x_val_norm, y_val_oh)  # Pass validation data\n",
        "\n",
        "# Evaluate the model (include test data)\n",
        "test_loss_lstm, test_acc_lstm = evaluate_model(lstm_model, x_test_norm, y_test_oh, is_test=True)  # Pass is_test=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zzwF7PB0fqa"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate CNN+LSTM\n",
        "cnn_lstm_model = CNNLSTMModel(win_size=x_train_norm.shape[1], num_channels=x_train_norm.shape[2], num_classes=y_train_oh.shape[1])\n",
        "\n",
        "# Train the model and store training history (include validation data)\n",
        "history_cnn_lstm = train_model(cnn_lstm_model, \"CNN+LSTM\", x_train_norm, y_train_oh, x_val_norm, y_val_oh)\n",
        "\n",
        "# Evaluate the model (include test data)\n",
        "test_loss_cnn_lstm, test_acc_cnn_lstm = evaluate_model(cnn_lstm_model, x_test_norm, y_test_oh, is_test=True)  # Pass is_test=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots training & validation accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "4_Oro4_VreC1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def plot_comparison(history_1dcnn, history_lstm, history_hybrid=None):\n",
        "    \"\"\"\n",
        "    Plots training & validation accuracy and loss for 1D CNN, LSTM, and optionally a CNN-LSTM hybrid model.\n",
        "    Organizes plots with training metrics in first row and validation metrics in second row.\n",
        "    Uses direct lines without markers for cleaner visualization.\n",
        "    \n",
        "    Parameters:\n",
        "    - history_1dcnn: Dict with 'train_loss', 'train_acc', 'val_loss', 'val_acc' for 1D CNN\n",
        "    - history_lstm: Dict with 'train_loss', 'train_acc', 'val_loss', 'val_acc' for LSTM\n",
        "    - history_hybrid: Optional dict for Hybrid model (CNN+LSTM)\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history_1dcnn['train_acc']) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(16, 10))\n",
        "    \n",
        "    # -------------------- TRAINING ACCURACY PLOT (Row 1, Column 1) --------------------\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, history_1dcnn['train_acc'], 'b-', label='1D CNN')\n",
        "    plt.plot(epochs, history_lstm['train_acc'], 'r-', label='LSTM')\n",
        "    \n",
        "    if history_hybrid is not None:\n",
        "        plt.plot(epochs, history_hybrid['train_acc'], 'g-', label='CNN-LSTM Hybrid')\n",
        "    \n",
        "    plt.title('Training Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # -------------------- TRAINING LOSS PLOT (Row 1, Column 2) --------------------\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, history_1dcnn['train_loss'], 'b-', label='1D CNN')\n",
        "    plt.plot(epochs, history_lstm['train_loss'], 'r-', label='LSTM')\n",
        "    \n",
        "    if history_hybrid is not None:\n",
        "        plt.plot(epochs, history_hybrid['train_loss'], 'g-', label='CNN-LSTM Hybrid')\n",
        "    \n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # -------------------- VALIDATION ACCURACY PLOT (Row 2, Column 1) --------------------\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, history_1dcnn['val_acc'], 'b-', label='1D CNN')\n",
        "    plt.plot(epochs, history_lstm['val_acc'], 'r-', label='LSTM')\n",
        "    \n",
        "    if history_hybrid is not None:\n",
        "        plt.plot(epochs, history_hybrid['val_acc'], 'g-', label='CNN-LSTM Hybrid')\n",
        "    \n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # -------------------- VALIDATION LOSS PLOT (Row 2, Column 2) --------------------\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, history_1dcnn['val_loss'], 'b-', label='1D CNN')\n",
        "    plt.plot(epochs, history_lstm['val_loss'], 'r-', label='LSTM')\n",
        "    \n",
        "    if history_hybrid is not None:\n",
        "        plt.plot(epochs, history_hybrid['val_loss'], 'g-', label='CNN-LSTM Hybrid')\n",
        "    \n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30xRFupAEVC4"
      },
      "outputs": [],
      "source": [
        "plot_comparison(history_cnn, history_lstm, history_cnn_lstm)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
