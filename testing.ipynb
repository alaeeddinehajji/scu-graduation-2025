{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import random\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set memory optimization flags and reproducibility\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for multimodal data - Fixed to load data to GPU only when needed\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, emg_data, eeg_data, labels):\n",
    "        self.emg_data = torch.FloatTensor(emg_data)\n",
    "        self.eeg_data = torch.FloatTensor(eeg_data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.emg_data[idx], self.eeg_data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the improved EMG encoder network\n",
    "class EMGEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EMGEncoder, self).__init__()\n",
    "        # Deeper architecture with multiple convolutional layers\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.conv3 = nn.Conv1d(hidden_dim*2, hidden_dim*2, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, channels, sequence_length]\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Both global average pooling and max pooling for better feature extraction\n",
    "        avg_pool = torch.mean(x, dim=2)\n",
    "        max_pool, _ = torch.max(x, dim=2)\n",
    "        \n",
    "        # Concatenate both pooling results\n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the improved EEG encoder network\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EEGEncoder, self).__init__()\n",
    "        # Deeper architecture with multiple convolutional layers\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.conv3 = nn.Conv1d(hidden_dim*2, hidden_dim*2, kernel_size=3, padding=1) \n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, channels, sequence_length]\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Both global average pooling and max pooling for better feature extraction\n",
    "        avg_pool = torch.mean(x, dim=2)\n",
    "        max_pool, _ = torch.max(x, dim=2)\n",
    "        \n",
    "        # Concatenate both pooling results\n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the improved multimodal fusion network\n",
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self, emg_input_dim, eeg_input_dim, hidden_dim, num_classes):\n",
    "        super(MultimodalNet, self).__init__()\n",
    "        \n",
    "        self.emg_encoder = EMGEncoder(emg_input_dim, hidden_dim)\n",
    "        self.eeg_encoder = EEGEncoder(eeg_input_dim, hidden_dim)\n",
    "        \n",
    "        # Calculate the size of concatenated features (doubled due to dual pooling)\n",
    "        concat_size = hidden_dim * 2 * 2 * 2  # 2 encoders x 2 feature sizes each x 2 pooling types\n",
    "        \n",
    "        # Improved fusion layer with additional layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(concat_size, hidden_dim * 4),\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, emg, eeg):\n",
    "        emg_features = self.emg_encoder(emg)\n",
    "        eeg_features = self.eeg_encoder(eeg)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat((emg_features, eeg_features), dim=1)\n",
    "        \n",
    "        # Pass through fusion layer\n",
    "        output = self.fusion(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to data for augmentation\n",
    "def add_noise(data, noise_factor=0.05):\n",
    "    noise = torch.randn(data.shape).to(data.device) * noise_factor * torch.std(data)\n",
    "    return data + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and load the data\n",
    "def load_and_process_data(emg_path, eeg_path, window_size=50, stride=25):\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data = pd.read_csv(emg_path)\n",
    "    eeg_data = pd.read_csv(eeg_path)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    emg_features = emg_data.iloc[:, :8].values\n",
    "    eeg_features = eeg_data.iloc[:, :8].values\n",
    "    \n",
    "    # Standardize the features\n",
    "    print(\"Normalizing data...\")\n",
    "    emg_scaler = StandardScaler()\n",
    "    eeg_scaler = StandardScaler()\n",
    "    \n",
    "    emg_features = emg_scaler.fit_transform(emg_features)\n",
    "    eeg_features = eeg_scaler.fit_transform(eeg_features)\n",
    "    \n",
    "    # Create windowed data\n",
    "    emg_windows = []\n",
    "    eeg_windows = []\n",
    "    window_labels = []\n",
    "    sample_ids = []  # Track sample IDs for stratified splits\n",
    "    \n",
    "    # Find common samples between EMG and EEG data\n",
    "    emg_samples = set(tuple(x) for x in emg_data[['subject', 'repetition', 'gesture']].drop_duplicates().values)\n",
    "    eeg_samples = set(tuple(x) for x in eeg_data[['subject', 'repetition', 'gesture']].drop_duplicates().values)\n",
    "    common_samples = emg_samples.intersection(eeg_samples)\n",
    "    \n",
    "    print(f\"Found {len(common_samples)} common samples between EMG and EEG data.\")\n",
    "    \n",
    "    for sample in common_samples:\n",
    "        subject, repetition, gesture = sample\n",
    "        \n",
    "        # Get data for this sample\n",
    "        emg_sample = emg_data[(emg_data['subject'] == subject) & \n",
    "                              (emg_data['repetition'] == repetition) & \n",
    "                              (emg_data['gesture'] == gesture)]\n",
    "        \n",
    "        eeg_sample = eeg_data[(eeg_data['subject'] == subject) & \n",
    "                              (eeg_data['repetition'] == repetition) & \n",
    "                              (eeg_data['gesture'] == gesture)]\n",
    "        \n",
    "        # Make sure both samples have data\n",
    "        if len(emg_sample) == 0 or len(eeg_sample) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Extract features\n",
    "        emg_sample_features = emg_sample.iloc[:, :8].values\n",
    "        eeg_sample_features = eeg_sample.iloc[:, :8].values\n",
    "        \n",
    "        # Standardize using pre-fitted scalers\n",
    "        emg_sample_features = emg_scaler.transform(emg_sample_features)\n",
    "        eeg_sample_features = eeg_scaler.transform(eeg_sample_features)\n",
    "        \n",
    "        # Handle different lengths by using the shorter one\n",
    "        min_length = min(len(emg_sample_features), len(eeg_sample_features))\n",
    "        if min_length <= window_size:\n",
    "            continue  # Skip if sample is too short\n",
    "            \n",
    "        emg_sample_features = emg_sample_features[:min_length]\n",
    "        eeg_sample_features = eeg_sample_features[:min_length]\n",
    "        \n",
    "        # Create windows\n",
    "        for i in range(0, min_length - window_size, stride):\n",
    "            emg_windows.append(emg_sample_features[i:i+window_size])\n",
    "            eeg_windows.append(eeg_sample_features[i:i+window_size])\n",
    "            window_labels.append(gesture - 1)  # 0-indexed labels\n",
    "            sample_ids.append(f\"{subject}_{repetition}_{gesture}\")  # Track which sample this comes from\n",
    "    \n",
    "    if len(emg_windows) == 0:\n",
    "        raise ValueError(\"No valid windows could be created. Check your data alignment.\")\n",
    "    \n",
    "    print(f\"Created {len(emg_windows)} windows from {len(common_samples)} samples.\")\n",
    "    \n",
    "    return np.array(emg_windows), np.array(eeg_windows), np.array(window_labels), np.array(sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with memory optimizations, mixed precision, and metrics tracking\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Keep track of metrics\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for i, (emg_inputs, eeg_inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            emg_inputs, eeg_inputs, labels = emg_inputs.to(device), eeg_inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Data augmentation (add noise) in training only\n",
    "            if random.random() < 0.5:  # 50% chance to apply noise\n",
    "                emg_inputs = add_noise(emg_inputs, noise_factor=0.03)\n",
    "                eeg_inputs = add_noise(eeg_inputs, noise_factor=0.03)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(emg_inputs, eeg_inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store predictions and labels for F1 score calculation\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Free up memory\n",
    "            if i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "        train_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Store metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for emg_inputs, eeg_inputs, labels in val_loader:\n",
    "                # Move data to device\n",
    "                emg_inputs, eeg_inputs, labels = emg_inputs.to(device), eeg_inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass (no mixed precision needed for validation)\n",
    "                outputs = model(emg_inputs, eeg_inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Free memory\n",
    "                del outputs, loss\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "        val_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        val_precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "        val_recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Store metrics\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n",
    "        print(f'Precision: {val_precision:.4f}, Recall: {val_recall:.4f}')\n",
    "        \n",
    "        # Save the best model based on F1 score - SIMPLIFIED to avoid serialization issues\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_acc = val_acc\n",
    "            \n",
    "            # SIMPLIFIED: Save only the model state dict\n",
    "            torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "            \n",
    "            # Save metrics separately\n",
    "            metrics = {\n",
    "                'epoch': epoch,\n",
    "                'val_f1': val_f1,\n",
    "                'val_acc': val_acc,\n",
    "            }\n",
    "            np.save('best_model_metrics.npy', metrics)\n",
    "            \n",
    "            # Save history separately\n",
    "            np.save('training_history.npy', history)\n",
    "            print(f'Saved model with F1: {val_f1:.4f}, Accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate on test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for emg_inputs, eeg_inputs, labels in test_loader:\n",
    "            # Move data to device\n",
    "            emg_inputs, eeg_inputs, labels = emg_inputs.to(device), eeg_inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(emg_inputs, eeg_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "    recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Make sure numpy is imported inside the function \n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import gc\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Hyperparameters\n",
    "    window_size = 50\n",
    "    batch_size = 32  # Adjusted batch size for mixed precision\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 3\n",
    "    hidden_dim = 64\n",
    "    \n",
    "    # Set paths to your data files\n",
    "    emg_path = 'data/processed/EMG-data.csv'\n",
    "    eeg_path = 'data/processed/EEG-data.csv'\n",
    "    \n",
    "    try:\n",
    "        # Load and process data\n",
    "        emg_windows, eeg_windows, window_labels, sample_ids = load_and_process_data(\n",
    "            emg_path, eeg_path, window_size=window_size\n",
    "        )\n",
    "        \n",
    "        # Calculate number of unique classes\n",
    "        num_classes = len(np.unique(window_labels))\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "        print(f\"EMG windows shape: {emg_windows.shape}\")\n",
    "        print(f\"EEG windows shape: {eeg_windows.shape}\")\n",
    "        \n",
    "        # Proper train/val/test split (60/20/20) - stratified by sample_id to prevent data leakage\n",
    "        unique_samples = np.unique(sample_ids)\n",
    "        \n",
    "        samples_train, samples_temp = train_test_split(\n",
    "            unique_samples, test_size=0.4, random_state=42\n",
    "        )\n",
    "        samples_val, samples_test = train_test_split(\n",
    "            samples_temp, test_size=0.5, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_mask = np.isin(sample_ids, samples_train)\n",
    "        val_mask = np.isin(sample_ids, samples_val)\n",
    "        test_mask = np.isin(sample_ids, samples_test)\n",
    "        \n",
    "        X_emg_train, X_eeg_train, y_train = emg_windows[train_mask], eeg_windows[train_mask], window_labels[train_mask]\n",
    "        X_emg_val, X_eeg_val, y_val = emg_windows[val_mask], eeg_windows[val_mask], window_labels[val_mask]\n",
    "        X_emg_test, X_eeg_test, y_test = emg_windows[test_mask], eeg_windows[test_mask], window_labels[test_mask]\n",
    "        \n",
    "        print(f\"Training set: {len(X_emg_train)} samples\")\n",
    "        print(f\"Validation set: {len(X_emg_val)} samples\")\n",
    "        print(f\"Test set: {len(X_emg_test)} samples\")\n",
    "        \n",
    "        # Free up memory\n",
    "        del emg_windows, eeg_windows, window_labels, sample_ids\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = MultimodalDataset(X_emg_train, X_eeg_train, y_train)\n",
    "        val_dataset = MultimodalDataset(X_emg_val, X_eeg_val, y_val)\n",
    "        test_dataset = MultimodalDataset(X_emg_test, X_eeg_test, y_test)\n",
    "        \n",
    "        # Free up more memory\n",
    "        del X_emg_train, X_emg_val, X_emg_test, X_eeg_train, X_eeg_val, X_eeg_test, y_train, y_val, y_test\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create data loaders with modified parameters\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, \n",
    "            pin_memory=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, shuffle=False,\n",
    "            pin_memory=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=False,\n",
    "            pin_memory=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        emg_input_dim = 8  # Number of EMG channels\n",
    "        eeg_input_dim = 8  # Number of EEG channels\n",
    "        \n",
    "        model = MultimodalNet(\n",
    "            emg_input_dim=emg_input_dim,\n",
    "            eeg_input_dim=eeg_input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Model has {total_params:,} parameters\")\n",
    "        \n",
    "        # Define optimizer and loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        \n",
    "        # Define cosine annealing learning rate scheduler\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=learning_rate/100)\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model, history = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        # FIXED: Simplified model loading approach\n",
    "        try:\n",
    "            # Only load the state dict, not the full checkpoint\n",
    "            state_dict = torch.load('best_multimodal_model.pth', map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(\"Successfully loaded model state dictionary\")\n",
    "            \n",
    "            # Load metrics separately if available\n",
    "            try:\n",
    "                metrics = np.load('best_model_metrics.npy', allow_pickle=True).item()\n",
    "                best_epoch = metrics.get('epoch', 0)\n",
    "                best_val_f1 = metrics.get('val_f1', 0.0)\n",
    "                best_val_acc = metrics.get('val_acc', 0.0)\n",
    "                \n",
    "                print(f\"Loaded best model from epoch {best_epoch+1} with validation F1: {best_val_f1:.4f}, \"\n",
    "                      f\"validation accuracy: {best_val_acc:.2f}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load metrics, but model weights were loaded successfully: {e}\")\n",
    "                best_epoch = 0\n",
    "                best_val_f1 = 0.0\n",
    "                best_val_acc = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Continuing with the current model state from the last epoch\")\n",
    "            best_epoch = num_epochs - 1\n",
    "            best_val_f1 = history['val_f1'][-1] if history['val_f1'] else 0.0\n",
    "            best_val_acc = history['val_acc'][-1] if history['val_acc'] else 0.0\n",
    "        \n",
    "        # Evaluate the model on test set\n",
    "        test_results = evaluate_model(model, test_loader)\n",
    "        \n",
    "        print(f\"Test F1 Score: {test_results['f1']:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_results['accuracy']:.2f}%\")\n",
    "        \n",
    "        # Save final results\n",
    "        final_results = {\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_f1': best_val_f1,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'test_results': test_results,\n",
    "            'history': history\n",
    "        }\n",
    "        \n",
    "        # Save as numpy file for later analysis\n",
    "        np.save('model_results.npy', final_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
