{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset from a CSV file.\n",
    "df = pd.read_csv(\"data/processed/EEG-data.csv\")\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def print_color(text: str, color: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints text in specified ANSI color for better readability.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be printed\n",
    "        color (str): Color name ('red', 'green', 'yellow', 'blue', 'magenta')\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        \"red\": \"\\033[91m\",\n",
    "        \"green\": \"\\033[92m\",\n",
    "        \"yellow\": \"\\033[93m\",\n",
    "        \"blue\": \"\\033[94m\",\n",
    "        \"magenta\": \"\\033[95m\"\n",
    "    }\n",
    "    print(f\"{colors.get(color, '')}{text}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_eeg_channels(data: np.ndarray, title: str = \"EEG Signals\", \n",
    "                     sample_idx: int = 0, channels: int = 8) -> None:\n",
    "    \"\"\"\n",
    "    Plot EEG channels from a window of data.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): EEG data array with shape (windows, time_steps, channels)\n",
    "        title (str): Title for the plot\n",
    "        sample_idx (int): Index of the window to plot\n",
    "        channels (int): Number of EEG channels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    window_data = data[sample_idx]\n",
    "    for i in range(channels):\n",
    "        plt.subplot(channels, 1, i + 1)\n",
    "        plt.plot(window_data[:, i])\n",
    "        plt.title(f'{title}, Channel {i+1}')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Amplitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust gesture labels to be zero-indexed\n",
    "df['gesture'] = df['gesture'] - 1\n",
    "\n",
    "# Count number of unique gestures\n",
    "num_classes = df['gesture'].nunique()\n",
    "\n",
    "# Display initial data and dataset structure\n",
    "print_color(\"Head of DataFrame:\", \"green\")\n",
    "print(df.head())\n",
    "print_color(\"Shape of DataFrame:\", \"green\")\n",
    "print(df.shape)\n",
    "\n",
    "# Check and display any null values in the dataset\n",
    "null_count = df.isnull().sum()\n",
    "print_color(\"Null values in each column:\", \"yellow\")\n",
    "print(null_count)\n",
    "\n",
    "# List and display unique gestures and subjects\n",
    "print_color(\"Unique gestures (after adjustment to 0-index):\", \"blue\")\n",
    "print(sorted(df[\"gesture\"].unique()))\n",
    "print_color(\"Unique subjects:\", \"blue\")\n",
    "print(sorted(df[\"subject\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Windowing Setup\n",
    "# Set window size and step size for slicing the data\n",
    "WINDOW_SIZE = 100  # Number of samples per window\n",
    "STEP_SIZE = 50     # Interval at which new windows are created\n",
    "\n",
    "# Initialize lists to store windowed data and corresponding labels\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Group the data by gesture, extracting features for each gesture\n",
    "for gesture_id in sorted(df[\"gesture\"].unique()):\n",
    "    gesture_df = df[df[\"gesture\"] == gesture_id]\n",
    "    gesture_data = gesture_df[\n",
    "        [\"Channel_1\", \"Channel_2\", \"Channel_3\", \"Channel_4\",\n",
    "         \"Channel_5\", \"Channel_6\", \"Channel_7\", \"Channel_8\"]\n",
    "    ].values  # Extract channel data as numpy array\n",
    "\n",
    "    # Generate overlapping windows of data\n",
    "    for start_idx in range(0, len(gesture_data) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        window_data = gesture_data[start_idx:start_idx + WINDOW_SIZE]\n",
    "        X_list.append(window_data)\n",
    "        y_list.append(gesture_id)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_array = np.array(X_list)\n",
    "y_array = np.array(y_list)\n",
    "\n",
    "\n",
    "# Display shapes of the prepared datasets\n",
    "print_color(\"Shape of X_array:\", \"red\")\n",
    "print(X_array.shape)\n",
    "print_color(\"Shape of y_array:\", \"red\")\n",
    "print(y_array.shape)\n",
    "\n",
    "# Print statistics about the data\n",
    "print_color(\"Data statistics in X_array:\", \"green\")\n",
    "print(\"Mean:\", np.mean(X_array, axis=(0, 1)))\n",
    "print(\"Standard Deviation:\", np.std(X_array, axis=(0, 1)))\n",
    "print(\"Max value:\", np.max(X_array))\n",
    "print(\"Min value:\", np.min(X_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the data\n",
    "X_mean = np.mean(X_array)\n",
    "X_std = np.std(X_array)\n",
    "X_array = (X_array - X_mean) / X_std\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "X_tensor = torch.FloatTensor(X_array).to(device)\n",
    "y_tensor = torch.LongTensor(y_array).to(device)\n",
    "\n",
    "# Print statistics about the tensor data\n",
    "print_color(\"Shape of X_tensor:\", \"red\")\n",
    "print(X_tensor.shape)\n",
    "print_color(\"Shape of y_tensor:\", \"red\")\n",
    "print(y_tensor.shape)\n",
    "print_color(\"Data statistics in X_tensor:\", \"green\")\n",
    "print(f\"Mean: {X_tensor.mean().item():.4f}\")\n",
    "print(f\"Standard Deviation: {X_tensor.std().item():.4f}\")\n",
    "print(f\"Max value: {X_tensor.max().item():.4f}\")\n",
    "print(f\"Min value: {X_tensor.min().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into train, validation, test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, random_state=RANDOM_SEED, stratify=y_tensor.cpu()\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train.cpu()\n",
    ")\n",
    "\n",
    "print_color(f\"Train set: {X_train.shape[0]} samples\", \"green\")\n",
    "print_color(f\"Validation set: {X_val.shape[0]} samples\", \"green\")\n",
    "print_color(f\"Test set: {X_test.shape[0]} samples\", \"green\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, win_size: int, num_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, 128, kernel_size=5, padding=2),  # Increased filters\n",
    "            nn.BatchNorm1d(128),  # Added batch norm\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),  # Added extra layer\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),  # Deeper architecture\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Better than fixed pooling\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),  # Increased dropout\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Permute to (batch_size, channels, time_steps)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, win_size, num_channels, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=num_channels, hidden_size=64, batch_first=True, bidirectional=False)\n",
    "        self.drop1 = nn.Dropout(0.2)  # Apply dropout after first LSTM\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True, bidirectional=False)\n",
    "        self.drop2 = nn.Dropout(0.2)  # Apply dropout after second LSTM\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.drop_fc = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.drop1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = x[:, -1, :]  # Take only the last time step output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, win_size, num_channels, num_classes):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "       \n",
    "        # --- CNN Block ---\n",
    "        self.cnn_block = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "           \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "       \n",
    "        # --- Attention Layer ---\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(256, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "       \n",
    "        # --- LSTM Block (Fixed Dropout) ---\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=128,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0  # Fixed: Set to 0 for a single-layer LSTM\n",
    "        )\n",
    "       \n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=256,  # 128*2 due to bidirectional\n",
    "            hidden_size=128,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0  # Fixed: Set to 0 for a single-layer LSTM\n",
    "        )\n",
    "       \n",
    "        # --- Fully Connected Layers ---\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(256, 512),  # 128*2 due to bidirectional\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # Reshape input for CNN: (batch_size, channels, time_steps)\n",
    "        x = x.permute(0, 2, 1)\n",
    "           \n",
    "        # CNN feature extraction\n",
    "        cnn_features = self.cnn_block(x)\n",
    "       \n",
    "        # Apply attention to CNN features\n",
    "        attention_weights = self.attention(cnn_features)\n",
    "        attended_features = cnn_features * attention_weights\n",
    "       \n",
    "        # Reshape for LSTM: (batch, time, features)\n",
    "        lstm_input = attended_features.permute(0, 2, 1)\n",
    "       \n",
    "        # LSTM processing\n",
    "        lstm_out1, _ = self.lstm1(lstm_input)\n",
    "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
    "       \n",
    "        # Global context representation (max + avg pooling)\n",
    "        max_pool = torch.max(lstm_out2, dim=1)[0]\n",
    "        avg_pool = torch.mean(lstm_out2, dim=1)\n",
    "        combined_features = max_pool + avg_pool\n",
    "       \n",
    "        # Final classification\n",
    "        output = self.fc_block(combined_features)\n",
    "       \n",
    "        return output\n",
    "   \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize model weights for better convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, \n",
    "               criterion: nn.Module, optimizer: torch.optim.Optimizer, \n",
    "               device: torch.device, epochs: int = 10) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train a model using the provided data loader.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        train_loader: DataLoader for training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer for updating model weights\n",
    "        device: Device to perform computations on (CPU/GPU)\n",
    "        epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        List of training loss values per epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': running_loss/total, \n",
    "                'acc': 100.*correct/total\n",
    "            })\n",
    "        \n",
    "        # Epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, \n",
    "                  criterion: nn.Module, device: torch.device) -> Tuple[float, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        device: Device to perform computations on (CPU/GPU)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (test loss, accuracy, confusion matrix)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Collect for confusion matrix\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(all_targets, all_preds))\n",
    "    \n",
    "    return test_loss, accuracy, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_training_results(losses: List[float], title: str = \"Training Loss\") -> None:\n",
    "    \"\"\"\n",
    "    Plot training results.\n",
    "    \n",
    "    Args:\n",
    "        losses: List of loss values per epoch\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, 'b-o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix: np.ndarray, class_names: List[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        conf_matrix: Confusion matrix array\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = [f'Gesture {i}' for i in range(conf_matrix.shape[0])]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# Loss function with class weights if needed\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create directory for models\n",
    "os.makedirs(\"models\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train CNN model\n",
    "print_color(\"\\nTraining CNN Model...\", \"magenta\")\n",
    "cnn_model = CNN1D(win_size=100, num_channels=8, num_classes=num_classes).to(device)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "start_time = time.time()\n",
    "cnn_losses = train_model(cnn_model, train_loader, criterion, cnn_optimizer, device, EPOCHS)\n",
    "cnn_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"CNN training completed in {cnn_train_time:.2f} seconds\")\n",
    "plot_training_results(cnn_losses, \"CNN Training Loss\")\n",
    "\n",
    "# Evaluate CNN model\n",
    "print_color(\"\\nEvaluating CNN Model...\", \"magenta\")\n",
    "cnn_test_loss, cnn_accuracy, cnn_conf_matrix = evaluate_model(cnn_model, test_loader, criterion, device)\n",
    "\n",
    "# Save CNN model\n",
    "torch.save(cnn_model.state_dict(), \"models/cnn_eeg_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train LSTM model\n",
    "print_color(\"\\nTraining LSTM Model...\", \"magenta\")\n",
    "lstm_model = LSTMModel(win_size=100, num_channels=8, num_classes=num_classes).to(device)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "start_time = time.time()\n",
    "lstm_losses = train_model(lstm_model, train_loader, criterion, lstm_optimizer, device, EPOCHS)\n",
    "lstm_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"LSTM training completed in {lstm_train_time:.2f} seconds\")\n",
    "plot_training_results(lstm_losses, \"LSTM Training Loss\")\n",
    "\n",
    "# Evaluate LSTM model\n",
    "print_color(\"\\nEvaluating LSTM Model...\", \"magenta\")\n",
    "lstm_test_loss, lstm_accuracy, lstm_conf_matrix = evaluate_model(lstm_model, test_loader, criterion, device)\n",
    "\n",
    "# Save LSTM model\n",
    "torch.save(lstm_model.state_dict(), \"models/lstm_eeg_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train CNN-LSTM hybrid model\n",
    "print_color(\"\\nTraining CNN-LSTM Hybrid Model...\", \"magenta\")\n",
    "cnn_lstm_model = CNNLSTMModel(win_size=100, num_channels=8, num_classes=num_classes).to(device)\n",
    "cnn_lstm_model.initialize_weights()\n",
    "cnn_lstm_optimizer = optim.Adam(cnn_lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "start_time = time.time()\n",
    "cnn_lstm_losses = train_model(cnn_lstm_model, train_loader, criterion, cnn_lstm_optimizer, device, EPOCHS)\n",
    "cnn_lstm_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"CNN-LSTM training completed in {cnn_lstm_train_time:.2f} seconds\")\n",
    "plot_training_results(cnn_lstm_losses, \"CNN-LSTM Training Loss\")\n",
    "\n",
    "# Evaluate CNN-LSTM model\n",
    "print_color(\"\\nEvaluating CNN-LSTM Hybrid Model...\", \"magenta\")\n",
    "cnn_lstm_test_loss, cnn_lstm_accuracy, cnn_lstm_conf_matrix = evaluate_model(cnn_lstm_model, test_loader, criterion, device)\n",
    "\n",
    "# Save CNN-LSTM model\n",
    "torch.save(cnn_lstm_model.state_dict(), \"models/cnn_lstm_eeg_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare model performances\n",
    "print_color(\"\\nModel Performance Comparison:\", \"blue\")\n",
    "models = [\"CNN\", \"LSTM\", \"CNN-LSTM\"]\n",
    "accuracies = [cnn_accuracy, lstm_accuracy, cnn_lstm_accuracy]\n",
    "train_times = [cnn_train_time, lstm_train_time, cnn_lstm_train_time]\n",
    "\n",
    "for model, acc, time_taken in zip(models, accuracies, train_times):\n",
    "    print(f\"{model}: Accuracy = {acc:.2f}%, Training Time = {time_taken:.2f} seconds\")\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(cnn_conf_matrix)\n",
    "plot_confusion_matrix(lstm_conf_matrix)\n",
    "plot_confusion_matrix(cnn_lstm_conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
