{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Multimodal EMG-EEG Gesture Classification using Deep Learning\n",
    "# This notebook implements a multimodal deep learning approach combining EMG and EEG data for gesture classification.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 1: Import Libraries and Set Environment\n",
    "# Import necessary libraries and configure environment settings\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import random\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set memory optimization flags and reproducibility\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 2: Configuration and Hyperparameters\n",
    "# Set up all configuration parameters, paths, and hyperparameters\n",
    "\n",
    "# %%\n",
    "# Data paths\n",
    "EMG_DATA_PATH = 'data/processed/EMG-data.csv'\n",
    "EEG_DATA_PATH = 'data/processed/EEG-data.csv'\n",
    "MODEL_SAVE_PATH = 'models/multimodal_eeg_emg.pth'\n",
    "\n",
    "# Data processing parameters\n",
    "DELTA_T = 35  # Time difference between EEG and EMG in ms\n",
    "WINDOW_SIZE = 200  # Window size for data processing\n",
    "\n",
    "# Model hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_LR = 0.001  # Initial learning rate\n",
    "MIN_LR = 1e-6      # Minimum learning rate\n",
    "WEIGHT_DECAY = 1e-5\n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "# Training settings\n",
    "TRAIN_TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Learning rate scheduler settings\n",
    "NUM_EPOCHS = 100  # Will run for full 100 epochs\n",
    "WARMUP_EPOCHS = 5\n",
    "CYCLES = 3  # Number of cosine annealing cycles\n",
    "CYCLE_LEN = NUM_EPOCHS // CYCLES  # Length of each cycle\n",
    "T_MULT = 2  # Factor to increase cycle length after each cycle\n",
    "ETA_MIN = MIN_LR  # Minimum learning rate for cosine annealing\n",
    "\n",
    "# Training improvements\n",
    "GRAD_CLIP_VAL = 1.0  # Gradient clipping value\n",
    "LABEL_SMOOTHING = 0.1  # Label smoothing factor\n",
    "NUM_CHECKPOINTS = 5  # Number of best checkpoints to save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 3: Helper Functions\n",
    "# Define data loading and training functions\n",
    "\n",
    "# %%\n",
    "def load_and_preprocess_data(emg_path, eeg_path, window_size=WINDOW_SIZE):\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data = pd.read_csv(emg_path)\n",
    "    eeg_data = pd.read_csv(eeg_path)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    emg_features = emg_data.iloc[:, :8].values  # 8 EMG channels\n",
    "    eeg_features = eeg_data.iloc[:, :8].values  # 8 EEG channels\n",
    "    \n",
    "    # Standardize features\n",
    "    print(\"Normalizing data...\")\n",
    "    emg_scaler = StandardScaler()\n",
    "    eeg_scaler = StandardScaler()\n",
    "    \n",
    "    emg_features = emg_scaler.fit_transform(emg_features)\n",
    "    eeg_features = eeg_scaler.fit_transform(eeg_features)\n",
    "    \n",
    "    # Create windowed data\n",
    "    emg_windows = []\n",
    "    eeg_windows = []\n",
    "    window_labels = []\n",
    "    sample_ids = []  # Track sample IDs for stratified splits\n",
    "    \n",
    "    # Find common samples between EMG and EEG data\n",
    "    emg_samples = set(tuple(x) for x in emg_data[['subject', 'repetition', 'gesture']].drop_duplicates().values)\n",
    "    eeg_samples = set(tuple(x) for x in eeg_data[['subject', 'repetition', 'gesture']].drop_duplicates().values)\n",
    "    common_samples = emg_samples.intersection(eeg_samples)\n",
    "    \n",
    "    print(f\"Found {len(common_samples)} common samples between EMG and EEG data.\")\n",
    "    \n",
    "    for sample in common_samples:\n",
    "        subject, repetition, gesture = sample\n",
    "        \n",
    "        # Get data for this sample\n",
    "        emg_sample = emg_data[(emg_data['subject'] == subject) & \n",
    "                             (emg_data['repetition'] == repetition) & \n",
    "                             (emg_data['gesture'] == gesture)]\n",
    "        \n",
    "        eeg_sample = eeg_data[(eeg_data['subject'] == subject) & \n",
    "                             (eeg_data['repetition'] == repetition) & \n",
    "                             (eeg_data['gesture'] == gesture)]\n",
    "        \n",
    "        # Make sure both samples have data\n",
    "        if len(emg_sample) == 0 or len(eeg_sample) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Extract features\n",
    "        emg_sample_features = emg_sample.iloc[:, :8].values\n",
    "        eeg_sample_features = eeg_sample.iloc[:, :8].values\n",
    "        \n",
    "        # Standardize using pre-fitted scalers\n",
    "        emg_sample_features = emg_scaler.transform(emg_sample_features)\n",
    "        eeg_sample_features = eeg_scaler.transform(eeg_sample_features)\n",
    "        \n",
    "        # Handle different lengths by using the shorter one\n",
    "        min_length = min(len(emg_sample_features), len(eeg_sample_features))\n",
    "        if min_length <= window_size:\n",
    "            continue  # Skip if sample is too short\n",
    "            \n",
    "        emg_sample_features = emg_sample_features[:min_length]\n",
    "        eeg_sample_features = eeg_sample_features[:min_length]\n",
    "        \n",
    "        # Create windows with fixed size\n",
    "        for i in range(0, min_length - window_size, window_size // 2):\n",
    "            emg_window = emg_sample_features[i:i + window_size]\n",
    "            eeg_window = eeg_sample_features[i:i + window_size]\n",
    "            \n",
    "            # Only add if window is complete\n",
    "            if len(emg_window) == window_size and len(eeg_window) == window_size:\n",
    "                # Transpose the windows to have shape (channels, time_steps)\n",
    "                emg_windows.append(emg_window.T)  # Shape: (8, window_size)\n",
    "                eeg_windows.append(eeg_window.T)  # Shape: (8, window_size)\n",
    "                window_labels.append(gesture - 1)  # 0-indexed labels\n",
    "                sample_ids.append(f\"{subject}_{repetition}_{gesture}\")\n",
    "    \n",
    "    if len(emg_windows) == 0:\n",
    "        raise ValueError(\"No valid windows could be created. Check your data alignment.\")\n",
    "    \n",
    "    print(f\"Created {len(emg_windows)} windows from {len(common_samples)} samples.\")\n",
    "    \n",
    "    # Convert to numpy arrays with explicit shape checking\n",
    "    emg_windows = np.array(emg_windows)  # Shape: (n_windows, n_channels, window_size)\n",
    "    eeg_windows = np.array(eeg_windows)  # Shape: (n_windows, n_channels, window_size)\n",
    "    window_labels = np.array(window_labels)\n",
    "    sample_ids = np.array(sample_ids)\n",
    "    \n",
    "    print(f\"EMG windows shape: {emg_windows.shape}\")\n",
    "    print(f\"EEG windows shape: {eeg_windows.shape}\")\n",
    "    \n",
    "    return emg_windows, eeg_windows, window_labels, sample_ids\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):\n",
    "    print(\"Starting training...\")\n",
    "    best_val_acc = 0.0\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for emg, eeg, labels in train_loader:\n",
    "            emg, eeg, labels = emg.to(device), eeg.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(emg, eeg)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for emg, eeg, labels in val_loader:\n",
    "                emg, eeg, labels = emg.to(device), eeg.to(device), labels.to(device)\n",
    "                outputs = model(emg, eeg)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 4: Dataset Class Definition\n",
    "# Implementation of the MultimodalDataset class for handling EMG and EEG data\n",
    "\n",
    "# %%\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, emg_data, eeg_data, labels, time_shift=DELTA_T):\n",
    "        # Data is already in shape (n_windows, n_channels, window_size)\n",
    "        self.emg_data = torch.FloatTensor(emg_data)\n",
    "        self.eeg_data = torch.FloatTensor(eeg_data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.time_shift = time_shift\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Shift EMG data by DELTA_T\n",
    "        emg = self.emg_data[idx]  # Shape: (n_channels, window_size)\n",
    "        eeg = self.eeg_data[idx]  # Shape: (n_channels, window_size)\n",
    "        if self.time_shift > 0:\n",
    "            emg = F.pad(emg[:, self.time_shift:], (0, self.time_shift))\n",
    "        return emg, eeg, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 5: Model Architecture Components\n",
    "# Define the CNN blocks and encoder components\n",
    "\n",
    "# %%\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(out_channels, out_channels*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class LateFusionCNN(nn.Module):\n",
    "    def __init__(self, emg_channels, eeg_channels, hidden_dim, num_classes):\n",
    "        super(LateFusionCNN, self).__init__()\n",
    "        \n",
    "        # EMG CNN Branch\n",
    "        self.emg_cnn = nn.Sequential(\n",
    "            CNNBlock(emg_channels, hidden_dim),\n",
    "            nn.Conv1d(hidden_dim*2, hidden_dim*4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # EEG CNN Branch\n",
    "        self.eeg_cnn = nn.Sequential(\n",
    "            CNNBlock(eeg_channels, hidden_dim),\n",
    "            nn.Conv1d(hidden_dim*2, hidden_dim*4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Late Fusion and Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*8, hidden_dim*4),\n",
    "            nn.LayerNorm(hidden_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim*4, hidden_dim*2),\n",
    "            nn.LayerNorm(hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim*2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, emg, eeg):\n",
    "        # Extract features from each modality\n",
    "        emg_features = self.emg_cnn(emg)  # Shape: [batch, hidden_dim*4]\n",
    "        eeg_features = self.eeg_cnn(eeg)  # Shape: [batch, hidden_dim*4]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat((emg_features, eeg_features), dim=1)  # Shape: [batch, hidden_dim*8]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 6: Main Model Architecture\n",
    "# Implementation of the CNNLSTMFusion model combining EMG and EEG features\n",
    "\n",
    "# %%\n",
    "class CNNLSTMFusion(nn.Module):\n",
    "    def __init__(self, emg_channels, eeg_channels, hidden_dim, num_classes):\n",
    "        super(CNNLSTMFusion, self).__init__()\n",
    "        \n",
    "        # CNN Encoders\n",
    "        self.emg_encoder = CNNEncoder(emg_channels, hidden_dim)\n",
    "        self.eeg_encoder = CNNEncoder(eeg_channels, hidden_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        lstm_input_dim = hidden_dim * 8 * 2  # Combined features from both modalities\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=hidden_dim * 4,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 8, hidden_dim * 4),\n",
    "            nn.LayerNorm(hidden_dim * 4),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, emg, eeg):\n",
    "        # Input shapes are already [batch, channels, sequence]\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        emg_features = self.emg_encoder(emg)\n",
    "        eeg_features = self.eeg_encoder(eeg)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat((emg_features, eeg_features), dim=1)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        batch_size = combined.size(0)\n",
    "        seq_len = combined.size(2)\n",
    "        combined = combined.permute(0, 2, 1)  # [batch, seq_len, features]\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        \n",
    "        # Take the last output\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(lstm_out)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 7: Data Loading and Preprocessing\n",
    "# Load and preprocess the EMG and EEG data\n",
    "\n",
    "# %%\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "emg_data, eeg_data, labels, sample_ids = load_and_preprocess_data(\n",
    "    EMG_DATA_PATH,\n",
    "    EEG_DATA_PATH\n",
    ")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_emg_train, X_emg_test, X_eeg_train, X_eeg_test, y_train, y_test = train_test_split(\n",
    "    emg_data, eeg_data, labels, \n",
    "    test_size=TRAIN_TEST_SPLIT, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "X_emg_train, X_emg_val, X_eeg_train, X_eeg_val, y_train, y_val = train_test_split(\n",
    "    X_emg_train, X_eeg_train, y_train, \n",
    "    test_size=VALIDATION_SPLIT, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"Training: EMG {X_emg_train.shape}, EEG {X_eeg_train.shape}\")\n",
    "print(f\"Validation: EMG {X_emg_val.shape}, EEG {X_eeg_val.shape}\")\n",
    "print(f\"Test: EMG {X_emg_test.shape}, EEG {X_eeg_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 8: Create Data Loaders\n",
    "# Prepare data loaders for training, validation, and testing\n",
    "\n",
    "# %%\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(X_emg_train, X_eeg_train, y_train)\n",
    "val_dataset = MultimodalDataset(X_emg_val, X_eeg_val, y_val)\n",
    "test_dataset = MultimodalDataset(X_emg_test, X_eeg_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(\"Data loaders created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 9: Model Initialization\n",
    "# Initialize the model, loss function, and optimizer with advanced learning rate scheduling\n",
    "\n",
    "# %%\n",
    "# Initialize model\n",
    "num_classes = len(np.unique(labels))\n",
    "model = LateFusionCNN(\n",
    "    emg_channels=8,\n",
    "    eeg_channels=8,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Define loss and optimizer with improved settings\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define learning rate scheduler with cosine annealing and warm restarts\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=CYCLE_LEN,  # First cycle length\n",
    "    T_mult=T_MULT,  # Cycle length multiplication factor\n",
    "    eta_min=MIN_LR  # Minimum learning rate\n",
    ")\n",
    "\n",
    "print(\"Model initialized successfully\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 10: Model Training\n",
    "# Train the model for full 100 epochs using advanced learning rate scheduling\n",
    "\n",
    "# %%\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'lr': [],\n",
    "    'val_f1': []  # Added F1 score tracking\n",
    "}\n",
    "\n",
    "# Keep track of best checkpoints\n",
    "best_checkpoints = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    # Warmup learning rate for first few epochs with cosine schedule\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        progress = epoch / WARMUP_EPOCHS\n",
    "        warmup_factor = 0.5 * (1 + np.cos(np.pi * (1 - progress)))\n",
    "        current_lr = INITIAL_LR * warmup_factor\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "    \n",
    "    # Training phase\n",
    "    for emg, eeg, labels in train_loader:\n",
    "        emg, eeg, labels = emg.to(device), eeg.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model(emg, eeg)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VAL)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for emg, eeg, labels in val_loader:\n",
    "            emg, eeg, labels = emg.to(device), eeg.to(device), labels.to(device)\n",
    "            outputs = model(emg, eeg)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update learning rate scheduler after warmup\n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}:')\n",
    "    print(f'LR: {current_lr:.6f}')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'val_f1': val_f1,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    # Update best checkpoints list\n",
    "    best_checkpoints.append((val_acc, epoch, checkpoint))\n",
    "    best_checkpoints.sort(reverse=True)  # Sort by validation accuracy\n",
    "    best_checkpoints = best_checkpoints[:NUM_CHECKPOINTS]  # Keep top N checkpoints\n",
    "    \n",
    "    # Save if it's among the best checkpoints\n",
    "    if (val_acc, epoch) >= (best_checkpoints[-1][0], best_checkpoints[-1][1]):\n",
    "        torch.save(checkpoint, f\"models/multimodal_eeg_emg/checkpoint_{len(best_checkpoints)}.pth\")\n",
    "        print(f'Saved checkpoint with validation accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save(checkpoint, MODEL_SAVE_PATH)\n",
    "            print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    print('-' * 60)\n",
    "\n",
    "print(f\"Best model was saved at epoch {best_epoch+1} with validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(\"\\nTop 5 best checkpoints:\")\n",
    "for i, (acc, epoch, _) in enumerate(best_checkpoints, 1):\n",
    "    print(f\"{i}. Epoch {epoch+1}: {acc:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))  # Adjusted figure size for better 2x2 layout\n",
    "\n",
    "# Plot accuracies (top left)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot losses (top right)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot F1 scores (bottom left)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history['val_f1'], label='Validation F1')\n",
    "plt.title('F1 Score History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "# Plot learning rate (bottom right)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history['lr'])\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Cell 11: Model Evaluation\n",
    "# Evaluate the best model on the test set\n",
    "\n",
    "# %%\n",
    "# Load best model and evaluate\n",
    "print(\"Evaluating best model on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH)['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for emg, eeg, labels in test_loader:\n",
    "        emg, eeg, labels = emg.to(device), eeg.to(device), labels.to(device)\n",
    "        outputs = model(emg, eeg)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = 100. * test_correct / test_total\n",
    "test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f'\\nTest Accuracy: {test_acc:.2f}%')\n",
    "print(f'Test F1 Score: {test_f1:.4f}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
