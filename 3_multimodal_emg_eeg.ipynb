{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Normalizing data...\n",
      "Found 378 common samples between EMG and EEG data.\n",
      "Created 19576 windows from 378 samples.\n",
      "Number of classes: 7\n",
      "EMG windows shape: (19576, 50, 8)\n",
      "EEG windows shape: (19576, 50, 8)\n",
      "Training set: 11752 samples\n",
      "Validation set: 3900 samples\n",
      "Test set: 3924 samples\n",
      "Model has 318,343 parameters\n",
      "Epoch 1/100, Train Loss: 1.0287, Train Acc: 62.66%, Train F1: 0.6237, Val Loss: 0.9673, Val Acc: 64.54%, Val F1: 0.6443\n",
      "Precision: 0.7500, Recall: 0.6454\n",
      "Saved model with F1: 0.6443, Accuracy: 64.54%\n",
      "Epoch 2/100, Train Loss: 0.5870, Train Acc: 79.68%, Train F1: 0.7957, Val Loss: 0.7110, Val Acc: 74.64%, Val F1: 0.7394\n",
      "Precision: 0.8068, Recall: 0.7464\n",
      "Saved model with F1: 0.7394, Accuracy: 74.64%\n",
      "Epoch 3/100, Train Loss: 0.4212, Train Acc: 85.59%, Train F1: 0.8550, Val Loss: 0.6426, Val Acc: 78.79%, Val F1: 0.7840\n",
      "Precision: 0.8349, Recall: 0.7879\n",
      "Saved model with F1: 0.7840, Accuracy: 78.79%\n",
      "Epoch 4/100, Train Loss: 0.3337, Train Acc: 88.44%, Train F1: 0.8838, Val Loss: 0.5752, Val Acc: 81.95%, Val F1: 0.8088\n",
      "Precision: 0.8511, Recall: 0.8195\n",
      "Saved model with F1: 0.8088, Accuracy: 81.95%\n",
      "Epoch 5/100, Train Loss: 0.2893, Train Acc: 90.27%, Train F1: 0.9022, Val Loss: 0.5737, Val Acc: 82.33%, Val F1: 0.8206\n",
      "Precision: 0.8661, Recall: 0.8233\n",
      "Saved model with F1: 0.8206, Accuracy: 82.33%\n",
      "Epoch 6/100, Train Loss: 0.2625, Train Acc: 91.23%, Train F1: 0.9119, Val Loss: 0.6056, Val Acc: 82.51%, Val F1: 0.8231\n",
      "Precision: 0.8493, Recall: 0.8251\n",
      "Saved model with F1: 0.8231, Accuracy: 82.51%\n",
      "Epoch 7/100, Train Loss: 0.2452, Train Acc: 91.75%, Train F1: 0.9172, Val Loss: 0.5420, Val Acc: 83.31%, Val F1: 0.8264\n",
      "Precision: 0.8582, Recall: 0.8331\n",
      "Saved model with F1: 0.8264, Accuracy: 83.31%\n",
      "Epoch 8/100, Train Loss: 0.2133, Train Acc: 92.78%, Train F1: 0.9276, Val Loss: 0.6034, Val Acc: 82.82%, Val F1: 0.8262\n",
      "Precision: 0.8613, Recall: 0.8282\n",
      "Epoch 9/100, Train Loss: 0.1978, Train Acc: 93.18%, Train F1: 0.9315, Val Loss: 0.4913, Val Acc: 85.33%, Val F1: 0.8500\n",
      "Precision: 0.8694, Recall: 0.8533\n",
      "Saved model with F1: 0.8500, Accuracy: 85.33%\n",
      "Epoch 10/100, Train Loss: 0.2000, Train Acc: 93.36%, Train F1: 0.9334, Val Loss: 0.4859, Val Acc: 86.05%, Val F1: 0.8600\n",
      "Precision: 0.8795, Recall: 0.8605\n",
      "Saved model with F1: 0.8600, Accuracy: 86.05%\n",
      "Epoch 11/100, Train Loss: 0.1903, Train Acc: 93.80%, Train F1: 0.9379, Val Loss: 0.4947, Val Acc: 84.10%, Val F1: 0.8388\n",
      "Precision: 0.8610, Recall: 0.8410\n",
      "Epoch 12/100, Train Loss: 0.1743, Train Acc: 93.79%, Train F1: 0.9377, Val Loss: 0.4698, Val Acc: 85.77%, Val F1: 0.8554\n",
      "Precision: 0.8755, Recall: 0.8577\n",
      "Epoch 13/100, Train Loss: 0.1589, Train Acc: 94.90%, Train F1: 0.9489, Val Loss: 0.4616, Val Acc: 87.36%, Val F1: 0.8707\n",
      "Precision: 0.8848, Recall: 0.8736\n",
      "Saved model with F1: 0.8707, Accuracy: 87.36%\n",
      "Epoch 14/100, Train Loss: 0.1543, Train Acc: 94.90%, Train F1: 0.9489, Val Loss: 0.5309, Val Acc: 86.31%, Val F1: 0.8608\n",
      "Precision: 0.8823, Recall: 0.8631\n",
      "Epoch 15/100, Train Loss: 0.1476, Train Acc: 95.04%, Train F1: 0.9503, Val Loss: 0.4905, Val Acc: 87.13%, Val F1: 0.8685\n",
      "Precision: 0.8857, Recall: 0.8713\n",
      "Epoch 16/100, Train Loss: 0.1411, Train Acc: 95.20%, Train F1: 0.9519, Val Loss: 0.4574, Val Acc: 87.72%, Val F1: 0.8768\n",
      "Precision: 0.8905, Recall: 0.8772\n",
      "Saved model with F1: 0.8768, Accuracy: 87.72%\n",
      "Epoch 17/100, Train Loss: 0.1445, Train Acc: 95.10%, Train F1: 0.9509, Val Loss: 0.4433, Val Acc: 88.03%, Val F1: 0.8769\n",
      "Precision: 0.8919, Recall: 0.8803\n",
      "Saved model with F1: 0.8769, Accuracy: 88.03%\n",
      "Epoch 18/100, Train Loss: 0.1261, Train Acc: 95.58%, Train F1: 0.9558, Val Loss: 0.4693, Val Acc: 87.92%, Val F1: 0.8783\n",
      "Precision: 0.8905, Recall: 0.8792\n",
      "Saved model with F1: 0.8783, Accuracy: 87.92%\n",
      "Epoch 19/100, Train Loss: 0.1190, Train Acc: 96.07%, Train F1: 0.9606, Val Loss: 0.4615, Val Acc: 88.82%, Val F1: 0.8862\n",
      "Precision: 0.9002, Recall: 0.8882\n",
      "Saved model with F1: 0.8862, Accuracy: 88.82%\n",
      "Epoch 20/100, Train Loss: 0.1205, Train Acc: 95.85%, Train F1: 0.9584, Val Loss: 0.5809, Val Acc: 85.87%, Val F1: 0.8556\n",
      "Precision: 0.8807, Recall: 0.8587\n",
      "Epoch 21/100, Train Loss: 0.1200, Train Acc: 96.10%, Train F1: 0.9609, Val Loss: 0.5156, Val Acc: 87.18%, Val F1: 0.8702\n",
      "Precision: 0.8876, Recall: 0.8718\n",
      "Epoch 22/100, Train Loss: 0.1025, Train Acc: 96.58%, Train F1: 0.9658, Val Loss: 0.4583, Val Acc: 88.72%, Val F1: 0.8852\n",
      "Precision: 0.8993, Recall: 0.8872\n",
      "Epoch 23/100, Train Loss: 0.1029, Train Acc: 96.38%, Train F1: 0.9637, Val Loss: 0.4810, Val Acc: 88.00%, Val F1: 0.8795\n",
      "Precision: 0.8965, Recall: 0.8800\n",
      "Epoch 24/100, Train Loss: 0.1032, Train Acc: 96.42%, Train F1: 0.9641, Val Loss: 0.5560, Val Acc: 87.21%, Val F1: 0.8702\n",
      "Precision: 0.8860, Recall: 0.8721\n",
      "Epoch 25/100, Train Loss: 0.0943, Train Acc: 96.95%, Train F1: 0.9694, Val Loss: 0.4756, Val Acc: 88.10%, Val F1: 0.8789\n",
      "Precision: 0.8906, Recall: 0.8810\n",
      "Epoch 26/100, Train Loss: 0.1012, Train Acc: 96.49%, Train F1: 0.9649, Val Loss: 0.5311, Val Acc: 88.64%, Val F1: 0.8854\n",
      "Precision: 0.8991, Recall: 0.8864\n",
      "Epoch 27/100, Train Loss: 0.1035, Train Acc: 96.70%, Train F1: 0.9669, Val Loss: 0.5419, Val Acc: 86.79%, Val F1: 0.8671\n",
      "Precision: 0.8848, Recall: 0.8679\n",
      "Epoch 28/100, Train Loss: 0.1010, Train Acc: 96.39%, Train F1: 0.9639, Val Loss: 0.5401, Val Acc: 87.62%, Val F1: 0.8732\n",
      "Precision: 0.8892, Recall: 0.8762\n",
      "Epoch 29/100, Train Loss: 0.0953, Train Acc: 96.84%, Train F1: 0.9684, Val Loss: 0.5346, Val Acc: 89.10%, Val F1: 0.8894\n",
      "Precision: 0.9016, Recall: 0.8910\n",
      "Saved model with F1: 0.8894, Accuracy: 89.10%\n",
      "Epoch 30/100, Train Loss: 0.0863, Train Acc: 97.14%, Train F1: 0.9714, Val Loss: 0.5009, Val Acc: 88.82%, Val F1: 0.8862\n",
      "Precision: 0.8991, Recall: 0.8882\n",
      "Epoch 31/100, Train Loss: 0.0856, Train Acc: 97.16%, Train F1: 0.9716, Val Loss: 0.4713, Val Acc: 88.90%, Val F1: 0.8869\n",
      "Precision: 0.9010, Recall: 0.8890\n",
      "Epoch 32/100, Train Loss: 0.0844, Train Acc: 97.33%, Train F1: 0.9733, Val Loss: 0.5986, Val Acc: 87.59%, Val F1: 0.8726\n",
      "Precision: 0.8905, Recall: 0.8759\n",
      "Epoch 33/100, Train Loss: 0.0755, Train Acc: 97.55%, Train F1: 0.9755, Val Loss: 0.4931, Val Acc: 90.46%, Val F1: 0.9041\n",
      "Precision: 0.9132, Recall: 0.9046\n",
      "Saved model with F1: 0.9041, Accuracy: 90.46%\n",
      "Epoch 34/100, Train Loss: 0.0740, Train Acc: 97.56%, Train F1: 0.9756, Val Loss: 0.5224, Val Acc: 89.85%, Val F1: 0.8973\n",
      "Precision: 0.9103, Recall: 0.8985\n",
      "Epoch 35/100, Train Loss: 0.0707, Train Acc: 97.52%, Train F1: 0.9752, Val Loss: 0.4522, Val Acc: 89.13%, Val F1: 0.8917\n",
      "Precision: 0.9000, Recall: 0.8913\n",
      "Epoch 36/100, Train Loss: 0.0641, Train Acc: 97.83%, Train F1: 0.9783, Val Loss: 0.5781, Val Acc: 87.67%, Val F1: 0.8746\n",
      "Precision: 0.8887, Recall: 0.8767\n",
      "Epoch 37/100, Train Loss: 0.0797, Train Acc: 97.36%, Train F1: 0.9736, Val Loss: 0.5544, Val Acc: 89.10%, Val F1: 0.8890\n",
      "Precision: 0.9023, Recall: 0.8910\n",
      "Epoch 38/100, Train Loss: 0.0736, Train Acc: 97.53%, Train F1: 0.9753, Val Loss: 0.5961, Val Acc: 89.13%, Val F1: 0.8901\n",
      "Precision: 0.9011, Recall: 0.8913\n",
      "Epoch 39/100, Train Loss: 0.0734, Train Acc: 97.55%, Train F1: 0.9755, Val Loss: 0.4907, Val Acc: 89.82%, Val F1: 0.8973\n",
      "Precision: 0.9081, Recall: 0.8982\n",
      "Epoch 40/100, Train Loss: 0.0621, Train Acc: 97.75%, Train F1: 0.9774, Val Loss: 0.5160, Val Acc: 89.44%, Val F1: 0.8942\n",
      "Precision: 0.9048, Recall: 0.8944\n",
      "Epoch 41/100, Train Loss: 0.0550, Train Acc: 98.22%, Train F1: 0.9822, Val Loss: 0.5634, Val Acc: 89.18%, Val F1: 0.8902\n",
      "Precision: 0.9029, Recall: 0.8918\n",
      "Epoch 42/100, Train Loss: 0.0551, Train Acc: 98.20%, Train F1: 0.9820, Val Loss: 0.5197, Val Acc: 89.41%, Val F1: 0.8928\n",
      "Precision: 0.9054, Recall: 0.8941\n",
      "Epoch 43/100, Train Loss: 0.0614, Train Acc: 98.04%, Train F1: 0.9804, Val Loss: 0.5148, Val Acc: 89.28%, Val F1: 0.8913\n",
      "Precision: 0.9034, Recall: 0.8928\n",
      "Epoch 44/100, Train Loss: 0.0615, Train Acc: 98.05%, Train F1: 0.9805, Val Loss: 0.5314, Val Acc: 88.38%, Val F1: 0.8828\n",
      "Precision: 0.8992, Recall: 0.8838\n",
      "Epoch 45/100, Train Loss: 0.0579, Train Acc: 98.14%, Train F1: 0.9814, Val Loss: 0.4495, Val Acc: 91.36%, Val F1: 0.9128\n",
      "Precision: 0.9204, Recall: 0.9136\n",
      "Saved model with F1: 0.9128, Accuracy: 91.36%\n",
      "Epoch 46/100, Train Loss: 0.0585, Train Acc: 98.00%, Train F1: 0.9800, Val Loss: 0.4408, Val Acc: 89.36%, Val F1: 0.8919\n",
      "Precision: 0.9036, Recall: 0.8936\n",
      "Epoch 47/100, Train Loss: 0.0513, Train Acc: 98.32%, Train F1: 0.9831, Val Loss: 0.5397, Val Acc: 89.03%, Val F1: 0.8880\n",
      "Precision: 0.9006, Recall: 0.8903\n",
      "Epoch 48/100, Train Loss: 0.0483, Train Acc: 98.43%, Train F1: 0.9843, Val Loss: 0.4340, Val Acc: 91.10%, Val F1: 0.9110\n",
      "Precision: 0.9196, Recall: 0.9110\n",
      "Epoch 49/100, Train Loss: 0.0436, Train Acc: 98.62%, Train F1: 0.9862, Val Loss: 0.5297, Val Acc: 90.28%, Val F1: 0.9032\n",
      "Precision: 0.9125, Recall: 0.9028\n",
      "Epoch 50/100, Train Loss: 0.0450, Train Acc: 98.52%, Train F1: 0.9852, Val Loss: 0.5854, Val Acc: 89.23%, Val F1: 0.8927\n",
      "Precision: 0.9044, Recall: 0.8923\n",
      "Epoch 51/100, Train Loss: 0.0529, Train Acc: 98.46%, Train F1: 0.9846, Val Loss: 0.4943, Val Acc: 91.23%, Val F1: 0.9112\n",
      "Precision: 0.9194, Recall: 0.9123\n",
      "Epoch 52/100, Train Loss: 0.0442, Train Acc: 98.50%, Train F1: 0.9850, Val Loss: 0.5527, Val Acc: 89.95%, Val F1: 0.8994\n",
      "Precision: 0.9114, Recall: 0.8995\n",
      "Epoch 53/100, Train Loss: 0.0375, Train Acc: 98.78%, Train F1: 0.9878, Val Loss: 0.5476, Val Acc: 89.23%, Val F1: 0.8892\n",
      "Precision: 0.9052, Recall: 0.8923\n",
      "Epoch 54/100, Train Loss: 0.0470, Train Acc: 98.37%, Train F1: 0.9837, Val Loss: 0.4440, Val Acc: 91.23%, Val F1: 0.9117\n",
      "Precision: 0.9190, Recall: 0.9123\n",
      "Epoch 55/100, Train Loss: 0.0355, Train Acc: 98.80%, Train F1: 0.9880, Val Loss: 0.4673, Val Acc: 90.97%, Val F1: 0.9091\n",
      "Precision: 0.9175, Recall: 0.9097\n",
      "Epoch 56/100, Train Loss: 0.0444, Train Acc: 98.64%, Train F1: 0.9864, Val Loss: 0.5314, Val Acc: 89.23%, Val F1: 0.8905\n",
      "Precision: 0.9010, Recall: 0.8923\n",
      "Epoch 57/100, Train Loss: 0.0395, Train Acc: 98.73%, Train F1: 0.9873, Val Loss: 0.6475, Val Acc: 87.72%, Val F1: 0.8764\n",
      "Precision: 0.8920, Recall: 0.8772\n",
      "Epoch 58/100, Train Loss: 0.0338, Train Acc: 98.88%, Train F1: 0.9888, Val Loss: 0.4764, Val Acc: 90.74%, Val F1: 0.9071\n",
      "Precision: 0.9149, Recall: 0.9074\n",
      "Epoch 59/100, Train Loss: 0.0356, Train Acc: 98.85%, Train F1: 0.9885, Val Loss: 0.5142, Val Acc: 90.10%, Val F1: 0.9007\n",
      "Precision: 0.9087, Recall: 0.9010\n",
      "Epoch 60/100, Train Loss: 0.0387, Train Acc: 98.72%, Train F1: 0.9872, Val Loss: 0.6574, Val Acc: 88.23%, Val F1: 0.8785\n",
      "Precision: 0.8952, Recall: 0.8823\n",
      "Epoch 61/100, Train Loss: 0.0312, Train Acc: 98.97%, Train F1: 0.9897, Val Loss: 0.5577, Val Acc: 91.00%, Val F1: 0.9090\n",
      "Precision: 0.9159, Recall: 0.9100\n",
      "Epoch 62/100, Train Loss: 0.0359, Train Acc: 98.81%, Train F1: 0.9881, Val Loss: 0.5199, Val Acc: 91.23%, Val F1: 0.9114\n",
      "Precision: 0.9200, Recall: 0.9123\n",
      "Epoch 63/100, Train Loss: 0.0412, Train Acc: 98.70%, Train F1: 0.9870, Val Loss: 0.5085, Val Acc: 91.92%, Val F1: 0.9187\n",
      "Precision: 0.9256, Recall: 0.9192\n",
      "Saved model with F1: 0.9187, Accuracy: 91.92%\n",
      "Epoch 64/100, Train Loss: 0.0339, Train Acc: 98.77%, Train F1: 0.9877, Val Loss: 0.7403, Val Acc: 87.67%, Val F1: 0.8750\n",
      "Precision: 0.8923, Recall: 0.8767\n",
      "Epoch 65/100, Train Loss: 0.0359, Train Acc: 98.93%, Train F1: 0.9893, Val Loss: 0.5703, Val Acc: 89.90%, Val F1: 0.8976\n",
      "Precision: 0.9078, Recall: 0.8990\n",
      "Epoch 66/100, Train Loss: 0.0305, Train Acc: 98.98%, Train F1: 0.9898, Val Loss: 0.5252, Val Acc: 91.26%, Val F1: 0.9119\n",
      "Precision: 0.9184, Recall: 0.9126\n",
      "Epoch 67/100, Train Loss: 0.0277, Train Acc: 99.17%, Train F1: 0.9917, Val Loss: 0.6145, Val Acc: 89.77%, Val F1: 0.8960\n",
      "Precision: 0.9077, Recall: 0.8977\n",
      "Epoch 68/100, Train Loss: 0.0289, Train Acc: 99.16%, Train F1: 0.9916, Val Loss: 0.5263, Val Acc: 91.18%, Val F1: 0.9118\n",
      "Precision: 0.9182, Recall: 0.9118\n",
      "Epoch 69/100, Train Loss: 0.0314, Train Acc: 99.09%, Train F1: 0.9909, Val Loss: 0.5802, Val Acc: 88.90%, Val F1: 0.8874\n",
      "Precision: 0.9000, Recall: 0.8890\n",
      "Epoch 70/100, Train Loss: 0.0238, Train Acc: 99.23%, Train F1: 0.9923, Val Loss: 0.5277, Val Acc: 90.87%, Val F1: 0.9078\n",
      "Precision: 0.9164, Recall: 0.9087\n",
      "Epoch 71/100, Train Loss: 0.0264, Train Acc: 99.17%, Train F1: 0.9917, Val Loss: 0.5221, Val Acc: 91.77%, Val F1: 0.9175\n",
      "Precision: 0.9224, Recall: 0.9177\n",
      "Epoch 72/100, Train Loss: 0.0244, Train Acc: 99.18%, Train F1: 0.9918, Val Loss: 0.5856, Val Acc: 91.05%, Val F1: 0.9096\n",
      "Precision: 0.9188, Recall: 0.9105\n",
      "Epoch 73/100, Train Loss: 0.0247, Train Acc: 99.22%, Train F1: 0.9922, Val Loss: 0.6038, Val Acc: 88.79%, Val F1: 0.8869\n",
      "Precision: 0.8986, Recall: 0.8879\n",
      "Epoch 74/100, Train Loss: 0.0256, Train Acc: 99.30%, Train F1: 0.9930, Val Loss: 0.5821, Val Acc: 90.03%, Val F1: 0.8992\n",
      "Precision: 0.9117, Recall: 0.9003\n",
      "Epoch 75/100, Train Loss: 0.0222, Train Acc: 99.29%, Train F1: 0.9929, Val Loss: 0.5475, Val Acc: 91.28%, Val F1: 0.9122\n",
      "Precision: 0.9204, Recall: 0.9128\n",
      "Epoch 76/100, Train Loss: 0.0231, Train Acc: 99.24%, Train F1: 0.9924, Val Loss: 0.5704, Val Acc: 90.87%, Val F1: 0.9079\n",
      "Precision: 0.9157, Recall: 0.9087\n",
      "Epoch 77/100, Train Loss: 0.0207, Train Acc: 99.29%, Train F1: 0.9929, Val Loss: 0.5250, Val Acc: 91.18%, Val F1: 0.9105\n",
      "Precision: 0.9193, Recall: 0.9118\n",
      "Epoch 78/100, Train Loss: 0.0232, Train Acc: 99.45%, Train F1: 0.9945, Val Loss: 0.7005, Val Acc: 89.08%, Val F1: 0.8900\n",
      "Precision: 0.9028, Recall: 0.8908\n",
      "Epoch 79/100, Train Loss: 0.0202, Train Acc: 99.30%, Train F1: 0.9930, Val Loss: 0.6865, Val Acc: 89.74%, Val F1: 0.8966\n",
      "Precision: 0.9104, Recall: 0.8974\n",
      "Epoch 80/100, Train Loss: 0.0198, Train Acc: 99.38%, Train F1: 0.9938, Val Loss: 0.5719, Val Acc: 90.46%, Val F1: 0.9044\n",
      "Precision: 0.9145, Recall: 0.9046\n",
      "Epoch 81/100, Train Loss: 0.0220, Train Acc: 99.28%, Train F1: 0.9928, Val Loss: 0.6070, Val Acc: 89.56%, Val F1: 0.8939\n",
      "Precision: 0.9061, Recall: 0.8956\n",
      "Epoch 82/100, Train Loss: 0.0198, Train Acc: 99.44%, Train F1: 0.9944, Val Loss: 0.5892, Val Acc: 90.85%, Val F1: 0.9072\n",
      "Precision: 0.9164, Recall: 0.9085\n",
      "Epoch 83/100, Train Loss: 0.0179, Train Acc: 99.44%, Train F1: 0.9944, Val Loss: 0.5720, Val Acc: 90.74%, Val F1: 0.9064\n",
      "Precision: 0.9160, Recall: 0.9074\n",
      "Epoch 84/100, Train Loss: 0.0212, Train Acc: 99.40%, Train F1: 0.9940, Val Loss: 0.5604, Val Acc: 91.05%, Val F1: 0.9100\n",
      "Precision: 0.9184, Recall: 0.9105\n",
      "Epoch 85/100, Train Loss: 0.0220, Train Acc: 99.31%, Train F1: 0.9931, Val Loss: 0.5262, Val Acc: 91.62%, Val F1: 0.9159\n",
      "Precision: 0.9231, Recall: 0.9162\n",
      "Epoch 86/100, Train Loss: 0.0180, Train Acc: 99.51%, Train F1: 0.9952, Val Loss: 0.5252, Val Acc: 91.67%, Val F1: 0.9161\n",
      "Precision: 0.9239, Recall: 0.9167\n",
      "Epoch 87/100, Train Loss: 0.0174, Train Acc: 99.49%, Train F1: 0.9949, Val Loss: 0.5416, Val Acc: 90.95%, Val F1: 0.9091\n",
      "Precision: 0.9181, Recall: 0.9095\n",
      "Epoch 88/100, Train Loss: 0.0155, Train Acc: 99.51%, Train F1: 0.9951, Val Loss: 0.5276, Val Acc: 91.28%, Val F1: 0.9125\n",
      "Precision: 0.9220, Recall: 0.9128\n",
      "Epoch 89/100, Train Loss: 0.0166, Train Acc: 99.48%, Train F1: 0.9948, Val Loss: 0.5076, Val Acc: 92.10%, Val F1: 0.9205\n",
      "Precision: 0.9276, Recall: 0.9210\n",
      "Saved model with F1: 0.9205, Accuracy: 92.10%\n",
      "Epoch 90/100, Train Loss: 0.0178, Train Acc: 99.49%, Train F1: 0.9949, Val Loss: 0.5744, Val Acc: 90.51%, Val F1: 0.9044\n",
      "Precision: 0.9155, Recall: 0.9051\n",
      "Epoch 91/100, Train Loss: 0.0192, Train Acc: 99.50%, Train F1: 0.9950, Val Loss: 0.5131, Val Acc: 91.59%, Val F1: 0.9154\n",
      "Precision: 0.9233, Recall: 0.9159\n",
      "Epoch 92/100, Train Loss: 0.0194, Train Acc: 99.34%, Train F1: 0.9934, Val Loss: 0.5913, Val Acc: 90.44%, Val F1: 0.9035\n",
      "Precision: 0.9142, Recall: 0.9044\n",
      "Epoch 93/100, Train Loss: 0.0145, Train Acc: 99.56%, Train F1: 0.9956, Val Loss: 0.5321, Val Acc: 91.59%, Val F1: 0.9153\n",
      "Precision: 0.9237, Recall: 0.9159\n",
      "Epoch 94/100, Train Loss: 0.0177, Train Acc: 99.50%, Train F1: 0.9950, Val Loss: 0.5449, Val Acc: 90.87%, Val F1: 0.9075\n",
      "Precision: 0.9168, Recall: 0.9087\n",
      "Epoch 95/100, Train Loss: 0.0153, Train Acc: 99.49%, Train F1: 0.9949, Val Loss: 0.5530, Val Acc: 91.23%, Val F1: 0.9109\n",
      "Precision: 0.9195, Recall: 0.9123\n",
      "Epoch 96/100, Train Loss: 0.0184, Train Acc: 99.56%, Train F1: 0.9956, Val Loss: 0.5579, Val Acc: 91.33%, Val F1: 0.9121\n",
      "Precision: 0.9216, Recall: 0.9133\n",
      "Epoch 97/100, Train Loss: 0.0162, Train Acc: 99.48%, Train F1: 0.9948, Val Loss: 0.5614, Val Acc: 90.69%, Val F1: 0.9060\n",
      "Precision: 0.9160, Recall: 0.9069\n",
      "Epoch 98/100, Train Loss: 0.0159, Train Acc: 99.47%, Train F1: 0.9947, Val Loss: 0.5101, Val Acc: 91.82%, Val F1: 0.9180\n",
      "Precision: 0.9246, Recall: 0.9182\n",
      "Epoch 99/100, Train Loss: 0.0160, Train Acc: 99.45%, Train F1: 0.9945, Val Loss: 0.5365, Val Acc: 90.92%, Val F1: 0.9085\n",
      "Precision: 0.9180, Recall: 0.9092\n",
      "Epoch 100/100, Train Loss: 0.0163, Train Acc: 99.57%, Train F1: 0.9957, Val Loss: 0.5097, Val Acc: 91.82%, Val F1: 0.9177\n",
      "Precision: 0.9248, Recall: 0.9182\n",
      "Training completed!\n",
      "Successfully loaded model state dictionary\n",
      "Loaded best model from epoch 89 with validation F1: 0.9205, validation accuracy: 92.10%\n",
      "Test Results:\n",
      "Accuracy: 86.70%\n",
      "F1 Score: 0.8562\n",
      "Precision: 0.8784\n",
      "Recall: 0.8670\n",
      "Confusion Matrix:\n",
      "[[588   1  12   0   1  22   0]\n",
      " [  0 520   0   0   0   0   0]\n",
      " [  0 180 267  55  16  10  68]\n",
      " [  0   6  18 447  22  52  27]\n",
      " [  0   0   0   0 466   2   0]\n",
      " [  0   0   0   0   1 657  18]\n",
      " [  0   0   0   0   3   8 457]]\n",
      "Test F1 Score: 0.8562\n",
      "Test Accuracy: 86.70%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import random\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set memory optimization flags and reproducibility\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a custom dataset for multimodal data - Fixed to load data to GPU only when needed\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, emg_data, eeg_data, labels):\n",
    "        self.emg_data = torch.FloatTensor(emg_data)\n",
    "        self.eeg_data = torch.FloatTensor(eeg_data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.emg_data[idx], self.eeg_data[idx], self.labels[idx]\n",
    "\n",
    "# Define the improved EMG encoder network\n",
    "class EMGEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EMGEncoder, self).__init__()\n",
    "        # Deeper architecture with multiple convolutional layers\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.conv3 = nn.Conv1d(hidden_dim*2, hidden_dim*2, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, channels, sequence_length]\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Both global average pooling and max pooling for better feature extraction\n",
    "        avg_pool = torch.mean(x, dim=2)\n",
    "        max_pool, _ = torch.max(x, dim=2)\n",
    "        \n",
    "        # Concatenate both pooling results\n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        return x\n",
    "\n",
    "# Define the improved EEG encoder network\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EEGEncoder, self).__init__()\n",
    "        # Deeper architecture with multiple convolutional layers\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.conv3 = nn.Conv1d(hidden_dim*2, hidden_dim*2, kernel_size=3, padding=1) \n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, channels, sequence_length]\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Both global average pooling and max pooling for better feature extraction\n",
    "        avg_pool = torch.mean(x, dim=2)\n",
    "        max_pool, _ = torch.max(x, dim=2)\n",
    "        \n",
    "        # Concatenate both pooling results\n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        return x\n",
    "\n",
    "# Define the improved multimodal fusion network\n",
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self, emg_input_dim, eeg_input_dim, hidden_dim, num_classes):\n",
    "        super(MultimodalNet, self).__init__()\n",
    "        \n",
    "        self.emg_encoder = EMGEncoder(emg_input_dim, hidden_dim)\n",
    "        self.eeg_encoder = EEGEncoder(eeg_input_dim, hidden_dim)\n",
    "        \n",
    "        # Calculate the size of concatenated features (doubled due to dual pooling)\n",
    "        concat_size = hidden_dim * 2 * 2 * 2  # 2 encoders x 2 feature sizes each x 2 pooling types\n",
    "        \n",
    "        # Improved fusion layer with additional layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(concat_size, hidden_dim * 4),\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, emg, eeg):\n",
    "        emg_features = self.emg_encoder(emg)\n",
    "        eeg_features = self.eeg_encoder(eeg)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat((emg_features, eeg_features), dim=1)\n",
    "        \n",
    "        # Pass through fusion layer\n",
    "        output = self.fusion(combined)\n",
    "        return output\n",
    "\n",
    "# Add noise to data for augmentation\n",
    "def add_noise(data, noise_factor=0.05):\n",
    "    noise = torch.randn(data.shape).to(data.device) * noise_factor * torch.std(data)\n",
    "    return data + noise\n",
    "\n",
    "# Process and load the data\n",
    "def load_and_process_data(emg_path, eeg_path, window_size=50, stride=25):\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data = pd.read_csv(emg_path)\n",
    "    eeg_data = pd.read_csv(eeg_path)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    emg_features = emg_data.iloc[:, :8].values\n",
    "    eeg_features = eeg_data.iloc[:, :8].values\n",
    "    \n",
    "    # Standardize the features\n",
    "    print(\"Normalizing data...\")\n",
    "    emg_scaler = StandardScaler()\n",
    "    eeg_scaler = StandardScaler()\n",
    "    \n",
    "    emg_features = emg_scaler.fit_transform(emg_features)\n",
    "    eeg_features = eeg_scaler.fit_transform(eeg_features)\n",
    "    \n",
    "    # Create windowed data\n",
    "    emg_windows = []\n",
    "    eeg_windows = []\n",
    "    window_labels = []\n",
    "    sample_ids = []  # Track sample IDs for stratified splits\n",
    "    \n",
    "    # Find common samples between EMG and EEG data\n",
    "    emg_samples = set(tuple(x) for x in emg_data[['subject', 'repetition', 'gesture']].drop_duplicates().values)\n",
    "    eeg_samples = set(tuple(x) for x in eeg_data[['subject', 'repetition', 'gesture']].drop_duplicates().values)\n",
    "    common_samples = emg_samples.intersection(eeg_samples)\n",
    "    \n",
    "    print(f\"Found {len(common_samples)} common samples between EMG and EEG data.\")\n",
    "    \n",
    "    for sample in common_samples:\n",
    "        subject, repetition, gesture = sample\n",
    "        \n",
    "        # Get data for this sample\n",
    "        emg_sample = emg_data[(emg_data['subject'] == subject) & \n",
    "                              (emg_data['repetition'] == repetition) & \n",
    "                              (emg_data['gesture'] == gesture)]\n",
    "        \n",
    "        eeg_sample = eeg_data[(eeg_data['subject'] == subject) & \n",
    "                              (eeg_data['repetition'] == repetition) & \n",
    "                              (eeg_data['gesture'] == gesture)]\n",
    "        \n",
    "        # Make sure both samples have data\n",
    "        if len(emg_sample) == 0 or len(eeg_sample) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Extract features\n",
    "        emg_sample_features = emg_sample.iloc[:, :8].values\n",
    "        eeg_sample_features = eeg_sample.iloc[:, :8].values\n",
    "        \n",
    "        # Standardize using pre-fitted scalers\n",
    "        emg_sample_features = emg_scaler.transform(emg_sample_features)\n",
    "        eeg_sample_features = eeg_scaler.transform(eeg_sample_features)\n",
    "        \n",
    "        # Handle different lengths by using the shorter one\n",
    "        min_length = min(len(emg_sample_features), len(eeg_sample_features))\n",
    "        if min_length <= window_size:\n",
    "            continue  # Skip if sample is too short\n",
    "            \n",
    "        emg_sample_features = emg_sample_features[:min_length]\n",
    "        eeg_sample_features = eeg_sample_features[:min_length]\n",
    "        \n",
    "        # Create windows\n",
    "        for i in range(0, min_length - window_size, stride):\n",
    "            emg_windows.append(emg_sample_features[i:i+window_size])\n",
    "            eeg_windows.append(eeg_sample_features[i:i+window_size])\n",
    "            window_labels.append(gesture - 1)  # 0-indexed labels\n",
    "            sample_ids.append(f\"{subject}_{repetition}_{gesture}\")  # Track which sample this comes from\n",
    "    \n",
    "    if len(emg_windows) == 0:\n",
    "        raise ValueError(\"No valid windows could be created. Check your data alignment.\")\n",
    "    \n",
    "    print(f\"Created {len(emg_windows)} windows from {len(common_samples)} samples.\")\n",
    "    \n",
    "    return np.array(emg_windows), np.array(eeg_windows), np.array(window_labels), np.array(sample_ids)\n",
    "\n",
    "# Training function with memory optimizations, mixed precision, and metrics tracking\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Keep track of metrics\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for i, (emg_inputs, eeg_inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            emg_inputs, eeg_inputs, labels = emg_inputs.to(device), eeg_inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Data augmentation (add noise) in training only\n",
    "            if random.random() < 0.5:  # 50% chance to apply noise\n",
    "                emg_inputs = add_noise(emg_inputs, noise_factor=0.03)\n",
    "                eeg_inputs = add_noise(eeg_inputs, noise_factor=0.03)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(emg_inputs, eeg_inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store predictions and labels for F1 score calculation\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Free up memory\n",
    "            if i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "        train_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Store metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for emg_inputs, eeg_inputs, labels in val_loader:\n",
    "                # Move data to device\n",
    "                emg_inputs, eeg_inputs, labels = emg_inputs.to(device), eeg_inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass (no mixed precision needed for validation)\n",
    "                outputs = model(emg_inputs, eeg_inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Free memory\n",
    "                del outputs, loss\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "        val_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        val_precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "        val_recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Store metrics\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n",
    "        print(f'Precision: {val_precision:.4f}, Recall: {val_recall:.4f}')\n",
    "        \n",
    "        # Save the best model based on F1 score - SIMPLIFIED to avoid serialization issues\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_acc = val_acc\n",
    "            \n",
    "            # SIMPLIFIED: Save only the model state dict\n",
    "            torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "            \n",
    "            # Save metrics separately\n",
    "            metrics = {\n",
    "                'epoch': epoch,\n",
    "                'val_f1': val_f1,\n",
    "                'val_acc': val_acc,\n",
    "            }\n",
    "            np.save('best_model_metrics.npy', metrics)\n",
    "            \n",
    "            # Save history separately\n",
    "            np.save('training_history.npy', history)\n",
    "            print(f'Saved model with F1: {val_f1:.4f}, Accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    return model, history\n",
    "# Function to evaluate on test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for emg_inputs, eeg_inputs, labels in test_loader:\n",
    "            # Move data to device\n",
    "            emg_inputs, eeg_inputs, labels = emg_inputs.to(device), eeg_inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(emg_inputs, eeg_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "    recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Make sure numpy is imported inside the function \n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import gc\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Hyperparameters\n",
    "    window_size = 50\n",
    "    batch_size = 32  # Adjusted batch size for mixed precision\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 100\n",
    "    hidden_dim = 64\n",
    "    \n",
    "    # Set paths to your data files\n",
    "    emg_path = 'data/processed/EMG-data.csv'\n",
    "    eeg_path = 'data/processed/EEG-data.csv'\n",
    "    \n",
    "    try:\n",
    "        # Load and process data\n",
    "        emg_windows, eeg_windows, window_labels, sample_ids = load_and_process_data(\n",
    "            emg_path, eeg_path, window_size=window_size\n",
    "        )\n",
    "        \n",
    "        # Calculate number of unique classes\n",
    "        num_classes = len(np.unique(window_labels))\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "        print(f\"EMG windows shape: {emg_windows.shape}\")\n",
    "        print(f\"EEG windows shape: {eeg_windows.shape}\")\n",
    "        \n",
    "        # Proper train/val/test split (60/20/20) - stratified by sample_id to prevent data leakage\n",
    "        unique_samples = np.unique(sample_ids)\n",
    "        \n",
    "        samples_train, samples_temp = train_test_split(\n",
    "            unique_samples, test_size=0.4, random_state=42\n",
    "        )\n",
    "        samples_val, samples_test = train_test_split(\n",
    "            samples_temp, test_size=0.5, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_mask = np.isin(sample_ids, samples_train)\n",
    "        val_mask = np.isin(sample_ids, samples_val)\n",
    "        test_mask = np.isin(sample_ids, samples_test)\n",
    "        \n",
    "        X_emg_train, X_eeg_train, y_train = emg_windows[train_mask], eeg_windows[train_mask], window_labels[train_mask]\n",
    "        X_emg_val, X_eeg_val, y_val = emg_windows[val_mask], eeg_windows[val_mask], window_labels[val_mask]\n",
    "        X_emg_test, X_eeg_test, y_test = emg_windows[test_mask], eeg_windows[test_mask], window_labels[test_mask]\n",
    "        \n",
    "        print(f\"Training set: {len(X_emg_train)} samples\")\n",
    "        print(f\"Validation set: {len(X_emg_val)} samples\")\n",
    "        print(f\"Test set: {len(X_emg_test)} samples\")\n",
    "        \n",
    "        # Free up memory\n",
    "        del emg_windows, eeg_windows, window_labels, sample_ids\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = MultimodalDataset(X_emg_train, X_eeg_train, y_train)\n",
    "        val_dataset = MultimodalDataset(X_emg_val, X_eeg_val, y_val)\n",
    "        test_dataset = MultimodalDataset(X_emg_test, X_eeg_test, y_test)\n",
    "        \n",
    "        # Free up more memory\n",
    "        del X_emg_train, X_emg_val, X_emg_test, X_eeg_train, X_eeg_val, X_eeg_test, y_train, y_val, y_test\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create data loaders with modified parameters\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, \n",
    "            pin_memory=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, shuffle=False,\n",
    "            pin_memory=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=False,\n",
    "            pin_memory=True, num_workers=0\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        emg_input_dim = 8  # Number of EMG channels\n",
    "        eeg_input_dim = 8  # Number of EEG channels\n",
    "        \n",
    "        model = MultimodalNet(\n",
    "            emg_input_dim=emg_input_dim,\n",
    "            eeg_input_dim=eeg_input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Model has {total_params:,} parameters\")\n",
    "        \n",
    "        # Define optimizer and loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        \n",
    "        # Define cosine annealing learning rate scheduler\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=learning_rate/100)\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model, history = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        # FIXED: Simplified model loading approach\n",
    "        try:\n",
    "            # Only load the state dict, not the full checkpoint\n",
    "            state_dict = torch.load('best_multimodal_model.pth', map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(\"Successfully loaded model state dictionary\")\n",
    "            \n",
    "            # Load metrics separately if available\n",
    "            try:\n",
    "                metrics = np.load('best_model_metrics.npy', allow_pickle=True).item()\n",
    "                best_epoch = metrics.get('epoch', 0)\n",
    "                best_val_f1 = metrics.get('val_f1', 0.0)\n",
    "                best_val_acc = metrics.get('val_acc', 0.0)\n",
    "                \n",
    "                print(f\"Loaded best model from epoch {best_epoch+1} with validation F1: {best_val_f1:.4f}, \"\n",
    "                      f\"validation accuracy: {best_val_acc:.2f}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load metrics, but model weights were loaded successfully: {e}\")\n",
    "                best_epoch = 0\n",
    "                best_val_f1 = 0.0\n",
    "                best_val_acc = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Continuing with the current model state from the last epoch\")\n",
    "            best_epoch = num_epochs - 1\n",
    "            best_val_f1 = history['val_f1'][-1] if history['val_f1'] else 0.0\n",
    "            best_val_acc = history['val_acc'][-1] if history['val_acc'] else 0.0\n",
    "        \n",
    "        # Evaluate the model on test set\n",
    "        test_results = evaluate_model(model, test_loader)\n",
    "        \n",
    "        print(f\"Test F1 Score: {test_results['f1']:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_results['accuracy']:.2f}%\")\n",
    "        \n",
    "        # Save final results\n",
    "        final_results = {\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_f1': best_val_f1,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'test_results': test_results,\n",
    "            'history': history\n",
    "        }\n",
    "        \n",
    "        # Save as numpy file for later analysis\n",
    "        np.save('model_results.npy', final_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
